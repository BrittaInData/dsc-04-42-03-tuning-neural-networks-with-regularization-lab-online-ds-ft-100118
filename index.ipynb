{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints / X\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors\n",
    "# Only keep the 2000 most common words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products / Y / Labels\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the product labels to numerical values\n",
    "encoder = preprocessing.LabelEncoder() \n",
    "encoder.fit(product)\n",
    "product_cat = encoder.transform(product) \n",
    "\n",
    "# Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, \n",
    "                                                    test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the Model Using a Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this block of code \n",
    "random.seed(123)\n",
    "\n",
    "train_final = X_train[1000:]\n",
    "val = X_train[:1000]\n",
    "\n",
    "label_train_final = y_train[1000:]\n",
    "label_val = y_train[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a neural network using Keras as described above\n",
    "\n",
    "# Initialize a sequential model with 3 layers\n",
    "model = models.Sequential()\n",
    "\n",
    "# Two hidden layers (one with 50 nodes, the other with 25) - using relu ...\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# ... and the final classification output layer - using softmax\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the model\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                100050    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 182       \n",
      "=================================================================\n",
      "Total params: 101,507\n",
      "Trainable params: 101,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 2s 270us/step - loss: 1.9544 - acc: 0.1525 - val_loss: 1.9356 - val_acc: 0.1490\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.9207 - acc: 0.1748 - val_loss: 1.9132 - val_acc: 0.1660\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8966 - acc: 0.2043 - val_loss: 1.8917 - val_acc: 0.1860\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.8723 - acc: 0.2265 - val_loss: 1.8673 - val_acc: 0.2090\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8451 - acc: 0.2479 - val_loss: 1.8393 - val_acc: 0.2380\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.8143 - acc: 0.2789 - val_loss: 1.8071 - val_acc: 0.2770\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.7793 - acc: 0.3153 - val_loss: 1.7704 - val_acc: 0.3170\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7394 - acc: 0.3593 - val_loss: 1.7291 - val_acc: 0.3490\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.6952 - acc: 0.4003 - val_loss: 1.6828 - val_acc: 0.3960\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.6466 - acc: 0.4448 - val_loss: 1.6334 - val_acc: 0.4370\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5950 - acc: 0.4808 - val_loss: 1.5812 - val_acc: 0.4760\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5414 - acc: 0.5133 - val_loss: 1.5276 - val_acc: 0.5070\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4866 - acc: 0.5401 - val_loss: 1.4738 - val_acc: 0.5340\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4323 - acc: 0.5620 - val_loss: 1.4205 - val_acc: 0.5510\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3789 - acc: 0.5847 - val_loss: 1.3684 - val_acc: 0.5620\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3269 - acc: 0.6029 - val_loss: 1.3189 - val_acc: 0.5780\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2772 - acc: 0.6161 - val_loss: 1.2728 - val_acc: 0.6120\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2309 - acc: 0.6364 - val_loss: 1.2275 - val_acc: 0.6210\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1865 - acc: 0.6492 - val_loss: 1.1863 - val_acc: 0.6270\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1453 - acc: 0.6603 - val_loss: 1.1477 - val_acc: 0.6370\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1066 - acc: 0.6688 - val_loss: 1.1124 - val_acc: 0.6400\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0711 - acc: 0.6752 - val_loss: 1.0797 - val_acc: 0.6490\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0378 - acc: 0.6831 - val_loss: 1.0490 - val_acc: 0.6660\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0068 - acc: 0.6916 - val_loss: 1.0216 - val_acc: 0.6720\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9782 - acc: 0.6968 - val_loss: 0.9957 - val_acc: 0.6870\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9512 - acc: 0.7032 - val_loss: 0.9713 - val_acc: 0.6870\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9265 - acc: 0.7077 - val_loss: 0.9484 - val_acc: 0.6970\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9035 - acc: 0.7140 - val_loss: 0.9287 - val_acc: 0.7010\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8825 - acc: 0.7181 - val_loss: 0.9102 - val_acc: 0.7020\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8621 - acc: 0.7244 - val_loss: 0.8922 - val_acc: 0.7130\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8437 - acc: 0.7281 - val_loss: 0.8754 - val_acc: 0.7130\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8262 - acc: 0.7303 - val_loss: 0.8612 - val_acc: 0.7190\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8097 - acc: 0.7363 - val_loss: 0.8453 - val_acc: 0.7220\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7941 - acc: 0.7405 - val_loss: 0.8334 - val_acc: 0.7260\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7794 - acc: 0.7451 - val_loss: 0.8220 - val_acc: 0.7240\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.7662 - acc: 0.7483 - val_loss: 0.8113 - val_acc: 0.7290\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7534 - acc: 0.7485 - val_loss: 0.7980 - val_acc: 0.7250\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7408 - acc: 0.7556 - val_loss: 0.7895 - val_acc: 0.7300\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7293 - acc: 0.7576 - val_loss: 0.7796 - val_acc: 0.7250\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7189 - acc: 0.7623 - val_loss: 0.7699 - val_acc: 0.7260\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7082 - acc: 0.7647 - val_loss: 0.7629 - val_acc: 0.7320\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6980 - acc: 0.7663 - val_loss: 0.7544 - val_acc: 0.7330\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6883 - acc: 0.7716 - val_loss: 0.7515 - val_acc: 0.7360\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6793 - acc: 0.7719 - val_loss: 0.7421 - val_acc: 0.7360\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6706 - acc: 0.7729 - val_loss: 0.7376 - val_acc: 0.7320\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6618 - acc: 0.7761 - val_loss: 0.7294 - val_acc: 0.7340\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6543 - acc: 0.7795 - val_loss: 0.7247 - val_acc: 0.7410\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6457 - acc: 0.7820 - val_loss: 0.7179 - val_acc: 0.7410\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6384 - acc: 0.7839 - val_loss: 0.7128 - val_acc: 0.7420\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6313 - acc: 0.7873 - val_loss: 0.7074 - val_acc: 0.7400\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6242 - acc: 0.7900 - val_loss: 0.7031 - val_acc: 0.7440\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6175 - acc: 0.7927 - val_loss: 0.7001 - val_acc: 0.7460\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6108 - acc: 0.7969 - val_loss: 0.6967 - val_acc: 0.7410\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6045 - acc: 0.7959 - val_loss: 0.6919 - val_acc: 0.7420\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.5981 - acc: 0.7981 - val_loss: 0.6872 - val_acc: 0.7440\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5919 - acc: 0.8007 - val_loss: 0.6860 - val_acc: 0.7480\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.5862 - acc: 0.8023 - val_loss: 0.6803 - val_acc: 0.7480\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5805 - acc: 0.8035 - val_loss: 0.6770 - val_acc: 0.7520\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.5749 - acc: 0.8067 - val_loss: 0.6733 - val_acc: 0.7450\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5693 - acc: 0.8087 - val_loss: 0.6716 - val_acc: 0.7490\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5638 - acc: 0.8087 - val_loss: 0.6690 - val_acc: 0.7520\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.5590 - acc: 0.8105 - val_loss: 0.6651 - val_acc: 0.7500\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5537 - acc: 0.8137 - val_loss: 0.6641 - val_acc: 0.7490\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5481 - acc: 0.8149 - val_loss: 0.6620 - val_acc: 0.7460\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.5435 - acc: 0.8176 - val_loss: 0.6583 - val_acc: 0.7490\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5384 - acc: 0.8179 - val_loss: 0.6574 - val_acc: 0.7540\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.5344 - acc: 0.8205 - val_loss: 0.6544 - val_acc: 0.7530\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5291 - acc: 0.8208 - val_loss: 0.6510 - val_acc: 0.7570\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5246 - acc: 0.8240 - val_loss: 0.6489 - val_acc: 0.7550\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.5201 - acc: 0.8240 - val_loss: 0.6473 - val_acc: 0.7580\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5158 - acc: 0.8279 - val_loss: 0.6463 - val_acc: 0.7580\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5111 - acc: 0.8279 - val_loss: 0.6436 - val_acc: 0.7580\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5072 - acc: 0.8292 - val_loss: 0.6422 - val_acc: 0.7540\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5025 - acc: 0.8315 - val_loss: 0.6438 - val_acc: 0.7530\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4985 - acc: 0.8339 - val_loss: 0.6414 - val_acc: 0.7620\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4944 - acc: 0.8343 - val_loss: 0.6388 - val_acc: 0.7580\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4904 - acc: 0.8368 - val_loss: 0.6367 - val_acc: 0.7560\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4863 - acc: 0.8400 - val_loss: 0.6355 - val_acc: 0.7550\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4823 - acc: 0.8408 - val_loss: 0.6359 - val_acc: 0.7590\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4783 - acc: 0.8420 - val_loss: 0.6342 - val_acc: 0.7580\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4745 - acc: 0.8415 - val_loss: 0.6337 - val_acc: 0.7630\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4707 - acc: 0.8448 - val_loss: 0.6314 - val_acc: 0.7580\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4670 - acc: 0.8448 - val_loss: 0.6317 - val_acc: 0.7560\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4638 - acc: 0.8467 - val_loss: 0.6289 - val_acc: 0.7620\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4601 - acc: 0.8504 - val_loss: 0.6284 - val_acc: 0.7560\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4565 - acc: 0.8500 - val_loss: 0.6265 - val_acc: 0.7580\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4530 - acc: 0.8500 - val_loss: 0.6278 - val_acc: 0.7640\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4491 - acc: 0.8521 - val_loss: 0.6315 - val_acc: 0.7570\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 0.4460 - acc: 0.8535 - val_loss: 0.6258 - val_acc: 0.7670\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4424 - acc: 0.8552 - val_loss: 0.6261 - val_acc: 0.7710\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4389 - acc: 0.8561 - val_loss: 0.6237 - val_acc: 0.7660\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.4359 - acc: 0.8575 - val_loss: 0.6237 - val_acc: 0.7710\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4329 - acc: 0.8573 - val_loss: 0.6216 - val_acc: 0.7690\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4290 - acc: 0.8589 - val_loss: 0.6212 - val_acc: 0.7680\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4257 - acc: 0.8616 - val_loss: 0.6269 - val_acc: 0.7640\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4229 - acc: 0.8617 - val_loss: 0.6211 - val_acc: 0.7620\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4195 - acc: 0.8613 - val_loss: 0.6212 - val_acc: 0.7610\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4164 - acc: 0.8628 - val_loss: 0.6280 - val_acc: 0.7650\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4129 - acc: 0.8657 - val_loss: 0.6192 - val_acc: 0.7730\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4104 - acc: 0.8657 - val_loss: 0.6212 - val_acc: 0.7640\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4072 - acc: 0.8661 - val_loss: 0.6203 - val_acc: 0.7640\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4042 - acc: 0.8684 - val_loss: 0.6188 - val_acc: 0.7670\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4015 - acc: 0.8696 - val_loss: 0.6211 - val_acc: 0.7670\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3982 - acc: 0.8697 - val_loss: 0.6216 - val_acc: 0.7710\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3955 - acc: 0.8715 - val_loss: 0.6181 - val_acc: 0.7730\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3923 - acc: 0.8729 - val_loss: 0.6203 - val_acc: 0.7710\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.3899 - acc: 0.8725 - val_loss: 0.6181 - val_acc: 0.7670\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3868 - acc: 0.8740 - val_loss: 0.6186 - val_acc: 0.7700\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3840 - acc: 0.8764 - val_loss: 0.6202 - val_acc: 0.7730\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.3813 - acc: 0.8784 - val_loss: 0.6206 - val_acc: 0.7690\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3790 - acc: 0.8791 - val_loss: 0.6180 - val_acc: 0.7720\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.3757 - acc: 0.8799 - val_loss: 0.6220 - val_acc: 0.7720\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3731 - acc: 0.8804 - val_loss: 0.6183 - val_acc: 0.7740\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.3702 - acc: 0.8824 - val_loss: 0.6213 - val_acc: 0.7740\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.3677 - acc: 0.8825 - val_loss: 0.6170 - val_acc: 0.7730\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3650 - acc: 0.8836 - val_loss: 0.6217 - val_acc: 0.7770\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3623 - acc: 0.8843 - val_loss: 0.6197 - val_acc: 0.7700\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3597 - acc: 0.8852 - val_loss: 0.6180 - val_acc: 0.7730\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3571 - acc: 0.8865 - val_loss: 0.6205 - val_acc: 0.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3545 - acc: 0.8872 - val_loss: 0.6200 - val_acc: 0.7730\n"
     ]
    }
   ],
   "source": [
    "# Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final, label_train_final,\n",
    "                      epochs=120,\n",
    "                      batch_size=256,\n",
    "                      validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 68us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 117us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35152224007447563, 0.8897333333651225]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6390811082522074, 0.7720000004768371]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. This because the training set is slightly different: we removed 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'lime', label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values, 'midnightblue', label='Validation Loss')\n",
    "\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'lime', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'midnightblue', label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 2s 216us/step - loss: 1.9642 - acc: 0.1455 - val_loss: 1.9521 - val_acc: 0.1590\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.9433 - acc: 0.1747 - val_loss: 1.9375 - val_acc: 0.1810\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9282 - acc: 0.1867 - val_loss: 1.9250 - val_acc: 0.1910\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9149 - acc: 0.1900 - val_loss: 1.9135 - val_acc: 0.1920\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.9021 - acc: 0.1947 - val_loss: 1.9019 - val_acc: 0.1940\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.8890 - acc: 0.1969 - val_loss: 1.8899 - val_acc: 0.1990\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8747 - acc: 0.2073 - val_loss: 1.8762 - val_acc: 0.2130\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.8586 - acc: 0.2211 - val_loss: 1.8603 - val_acc: 0.2220\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.8393 - acc: 0.2409 - val_loss: 1.8402 - val_acc: 0.2380\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.8159 - acc: 0.2597 - val_loss: 1.8153 - val_acc: 0.2690\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7876 - acc: 0.2949 - val_loss: 1.7854 - val_acc: 0.3020\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7543 - acc: 0.3340 - val_loss: 1.7508 - val_acc: 0.3460\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.7169 - acc: 0.3775 - val_loss: 1.7144 - val_acc: 0.3630\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.6764 - acc: 0.4103 - val_loss: 1.6740 - val_acc: 0.3890\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6333 - acc: 0.4429 - val_loss: 1.6315 - val_acc: 0.4270\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.5882 - acc: 0.4740 - val_loss: 1.5870 - val_acc: 0.4570\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.5411 - acc: 0.5031 - val_loss: 1.5415 - val_acc: 0.5020\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.4930 - acc: 0.5361 - val_loss: 1.4948 - val_acc: 0.5250\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4438 - acc: 0.5671 - val_loss: 1.4479 - val_acc: 0.5370\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3945 - acc: 0.5852 - val_loss: 1.4016 - val_acc: 0.5690\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3458 - acc: 0.6065 - val_loss: 1.3558 - val_acc: 0.5920\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2987 - acc: 0.6287 - val_loss: 1.3122 - val_acc: 0.5850\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2529 - acc: 0.6392 - val_loss: 1.2683 - val_acc: 0.6290\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2086 - acc: 0.6567 - val_loss: 1.2272 - val_acc: 0.6310\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1664 - acc: 0.6675 - val_loss: 1.1868 - val_acc: 0.6430\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1265 - acc: 0.6765 - val_loss: 1.1503 - val_acc: 0.6510\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0886 - acc: 0.6867 - val_loss: 1.1147 - val_acc: 0.6530\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0536 - acc: 0.6913 - val_loss: 1.0836 - val_acc: 0.6600\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0206 - acc: 0.6992 - val_loss: 1.0532 - val_acc: 0.6600\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9900 - acc: 0.7019 - val_loss: 1.0248 - val_acc: 0.6730\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9614 - acc: 0.7076 - val_loss: 1.0010 - val_acc: 0.6750\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9344 - acc: 0.7133 - val_loss: 0.9761 - val_acc: 0.6860\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9105 - acc: 0.7196 - val_loss: 0.9524 - val_acc: 0.6820\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8868 - acc: 0.7249 - val_loss: 0.9311 - val_acc: 0.6880\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8649 - acc: 0.7305 - val_loss: 0.9122 - val_acc: 0.6950\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8449 - acc: 0.7345 - val_loss: 0.8958 - val_acc: 0.6940\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8261 - acc: 0.7379 - val_loss: 0.8785 - val_acc: 0.7040\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8084 - acc: 0.7425 - val_loss: 0.8630 - val_acc: 0.7120\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.7916 - acc: 0.7467 - val_loss: 0.8477 - val_acc: 0.7130\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.7758 - acc: 0.7488 - val_loss: 0.8344 - val_acc: 0.7220\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7618 - acc: 0.7531 - val_loss: 0.8226 - val_acc: 0.7190\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7470 - acc: 0.7548 - val_loss: 0.8114 - val_acc: 0.7280\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7344 - acc: 0.7569 - val_loss: 0.7994 - val_acc: 0.7300\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.7217 - acc: 0.7613 - val_loss: 0.7924 - val_acc: 0.7320\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7105 - acc: 0.7649 - val_loss: 0.7822 - val_acc: 0.7290\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.6995 - acc: 0.7672 - val_loss: 0.7728 - val_acc: 0.7290\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6886 - acc: 0.7696 - val_loss: 0.7640 - val_acc: 0.7350\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6787 - acc: 0.7717 - val_loss: 0.7575 - val_acc: 0.7330\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.6703 - acc: 0.775 - 0s 53us/step - loss: 0.6688 - acc: 0.7753 - val_loss: 0.7501 - val_acc: 0.7350\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6600 - acc: 0.7776 - val_loss: 0.7406 - val_acc: 0.7360\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6511 - acc: 0.7824 - val_loss: 0.7392 - val_acc: 0.7360\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6428 - acc: 0.7823 - val_loss: 0.7304 - val_acc: 0.7450\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6345 - acc: 0.7843 - val_loss: 0.7226 - val_acc: 0.7420\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6271 - acc: 0.7887 - val_loss: 0.7214 - val_acc: 0.7330\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.6198 - acc: 0.7899 - val_loss: 0.7139 - val_acc: 0.7480\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6123 - acc: 0.7919 - val_loss: 0.7123 - val_acc: 0.7480\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6053 - acc: 0.7948 - val_loss: 0.7057 - val_acc: 0.7450\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5982 - acc: 0.7965 - val_loss: 0.7030 - val_acc: 0.7490\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5925 - acc: 0.7964 - val_loss: 0.6980 - val_acc: 0.7490\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5862 - acc: 0.7997 - val_loss: 0.6939 - val_acc: 0.7540\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                        label_train_final,\n",
    "                        epochs=60,\n",
    "                        batch_size=256,\n",
    "                        validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 74us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 96us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.580313407643636, 0.8026666666984558]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7219348975817362, 0.7393333328564962]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 2.6170 - acc: 0.1436 - val_loss: 2.6069 - val_acc: 0.1670\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 2.5879 - acc: 0.1644 - val_loss: 2.5852 - val_acc: 0.1880\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 2.5664 - acc: 0.1811 - val_loss: 2.5662 - val_acc: 0.1980\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 2.5462 - acc: 0.2025 - val_loss: 2.5471 - val_acc: 0.2170\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 2.5245 - acc: 0.2293 - val_loss: 2.5256 - val_acc: 0.2540\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.5000 - acc: 0.2539 - val_loss: 2.5012 - val_acc: 0.2840\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 2.4724 - acc: 0.2780 - val_loss: 2.4732 - val_acc: 0.3130\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 2.4407 - acc: 0.3177 - val_loss: 2.4412 - val_acc: 0.3360\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.4041 - acc: 0.3535 - val_loss: 2.4037 - val_acc: 0.3750\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.3620 - acc: 0.3933 - val_loss: 2.3609 - val_acc: 0.4060\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 2.3142 - acc: 0.4332 - val_loss: 2.3122 - val_acc: 0.4440\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 2.2622 - acc: 0.4689 - val_loss: 2.2595 - val_acc: 0.4810\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 2.2065 - acc: 0.5040 - val_loss: 2.2035 - val_acc: 0.5040\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 2.1490 - acc: 0.5348 - val_loss: 2.1460 - val_acc: 0.5350\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.0896 - acc: 0.5633 - val_loss: 2.0867 - val_acc: 0.5680\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 2.0291 - acc: 0.5879 - val_loss: 2.0271 - val_acc: 0.5880\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.9695 - acc: 0.6119 - val_loss: 1.9702 - val_acc: 0.6120\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.9118 - acc: 0.6316 - val_loss: 1.9143 - val_acc: 0.6290\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8566 - acc: 0.6528 - val_loss: 1.8608 - val_acc: 0.6450\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8043 - acc: 0.6653 - val_loss: 1.8101 - val_acc: 0.6650\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.7549 - acc: 0.6841 - val_loss: 1.7644 - val_acc: 0.6730\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.7091 - acc: 0.6924 - val_loss: 1.7190 - val_acc: 0.6820\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6662 - acc: 0.7017 - val_loss: 1.6797 - val_acc: 0.6870\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.6267 - acc: 0.7092 - val_loss: 1.6414 - val_acc: 0.6940\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.5909 - acc: 0.7151 - val_loss: 1.6065 - val_acc: 0.6950\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.5574 - acc: 0.7213 - val_loss: 1.5764 - val_acc: 0.7070\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.5268 - acc: 0.7285 - val_loss: 1.5484 - val_acc: 0.7100\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.4988 - acc: 0.7304 - val_loss: 1.5221 - val_acc: 0.7140\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.4729 - acc: 0.7365 - val_loss: 1.4963 - val_acc: 0.7190\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4491 - acc: 0.7420 - val_loss: 1.4750 - val_acc: 0.7230\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.4271 - acc: 0.7431 - val_loss: 1.4546 - val_acc: 0.7240\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.4066 - acc: 0.7471 - val_loss: 1.4363 - val_acc: 0.7260\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3878 - acc: 0.7493 - val_loss: 1.4177 - val_acc: 0.7360\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3699 - acc: 0.7551 - val_loss: 1.4040 - val_acc: 0.7200\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3532 - acc: 0.7585 - val_loss: 1.3894 - val_acc: 0.7300\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3375 - acc: 0.7605 - val_loss: 1.3761 - val_acc: 0.7310\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.3231 - acc: 0.7633 - val_loss: 1.3608 - val_acc: 0.7400\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3087 - acc: 0.7679 - val_loss: 1.3508 - val_acc: 0.7370\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2960 - acc: 0.7692 - val_loss: 1.3386 - val_acc: 0.7350\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2834 - acc: 0.7719 - val_loss: 1.3273 - val_acc: 0.7450\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2715 - acc: 0.7757 - val_loss: 1.3181 - val_acc: 0.7500\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2598 - acc: 0.7773 - val_loss: 1.3071 - val_acc: 0.7560\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2490 - acc: 0.7827 - val_loss: 1.2986 - val_acc: 0.7540\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2388 - acc: 0.7833 - val_loss: 1.2891 - val_acc: 0.7540\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2280 - acc: 0.7840 - val_loss: 1.2832 - val_acc: 0.7490\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2184 - acc: 0.7869 - val_loss: 1.2736 - val_acc: 0.7570\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2095 - acc: 0.7896 - val_loss: 1.2657 - val_acc: 0.7600\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.2000 - acc: 0.7925 - val_loss: 1.2592 - val_acc: 0.7630\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.1910 - acc: 0.7959 - val_loss: 1.2529 - val_acc: 0.7590\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 1s 178us/step - loss: 1.1820 - acc: 0.7971 - val_loss: 1.2481 - val_acc: 0.7560\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 2s 216us/step - loss: 1.1739 - acc: 0.8021 - val_loss: 1.2397 - val_acc: 0.7610\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 1.1657 - acc: 0.8028 - val_loss: 1.2338 - val_acc: 0.7650\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.1582 - acc: 0.8024 - val_loss: 1.2286 - val_acc: 0.7640\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.1503 - acc: 0.8040 - val_loss: 1.2218 - val_acc: 0.7690\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 1.1427 - acc: 0.8084 - val_loss: 1.2173 - val_acc: 0.7660\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.1356 - acc: 0.8105 - val_loss: 1.2129 - val_acc: 0.7630\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.1282 - acc: 0.8105 - val_loss: 1.2081 - val_acc: 0.7700\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.1210 - acc: 0.8124 - val_loss: 1.2013 - val_acc: 0.7650\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.1142 - acc: 0.8160 - val_loss: 1.1977 - val_acc: 0.7620\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1074 - acc: 0.8151 - val_loss: 1.1923 - val_acc: 0.7660\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.1012 - acc: 0.8185 - val_loss: 1.1879 - val_acc: 0.7660\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0942 - acc: 0.8185 - val_loss: 1.1843 - val_acc: 0.7740\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.0876 - acc: 0.8200 - val_loss: 1.1829 - val_acc: 0.7630\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0818 - acc: 0.8221 - val_loss: 1.1753 - val_acc: 0.7750\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0754 - acc: 0.8227 - val_loss: 1.1719 - val_acc: 0.7800\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.0692 - acc: 0.8253 - val_loss: 1.1651 - val_acc: 0.7790\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.0628 - acc: 0.8293 - val_loss: 1.1642 - val_acc: 0.7750\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.0572 - acc: 0.8288 - val_loss: 1.1589 - val_acc: 0.7710\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.0517 - acc: 0.8308 - val_loss: 1.1571 - val_acc: 0.7790\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 1.0459 - acc: 0.8317 - val_loss: 1.1520 - val_acc: 0.7750\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 1.0403 - acc: 0.8328 - val_loss: 1.1481 - val_acc: 0.7770\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 1.0344 - acc: 0.8360 - val_loss: 1.1454 - val_acc: 0.7780\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.0288 - acc: 0.8327 - val_loss: 1.1452 - val_acc: 0.7760\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0239 - acc: 0.8377 - val_loss: 1.1424 - val_acc: 0.7770\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0184 - acc: 0.8376 - val_loss: 1.1355 - val_acc: 0.7820\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 1.0130 - acc: 0.8409 - val_loss: 1.1354 - val_acc: 0.7830\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 1.0079 - acc: 0.8416 - val_loss: 1.1300 - val_acc: 0.7830\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.0028 - acc: 0.8424 - val_loss: 1.1258 - val_acc: 0.7800\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 0.9978 - acc: 0.8448 - val_loss: 1.1224 - val_acc: 0.7830\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 0.9928 - acc: 0.8465 - val_loss: 1.1211 - val_acc: 0.7830\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 0.9877 - acc: 0.8481 - val_loss: 1.1166 - val_acc: 0.7800\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9830 - acc: 0.8481 - val_loss: 1.1150 - val_acc: 0.7840\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9783 - acc: 0.8511 - val_loss: 1.1129 - val_acc: 0.7800\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9734 - acc: 0.8511 - val_loss: 1.1087 - val_acc: 0.7810\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.9688 - acc: 0.8529 - val_loss: 1.1062 - val_acc: 0.7860\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9638 - acc: 0.8545 - val_loss: 1.1031 - val_acc: 0.7870\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9595 - acc: 0.8561 - val_loss: 1.1019 - val_acc: 0.7900\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9547 - acc: 0.8569 - val_loss: 1.0985 - val_acc: 0.7880\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9504 - acc: 0.8564 - val_loss: 1.0950 - val_acc: 0.7860\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9458 - acc: 0.8580 - val_loss: 1.0941 - val_acc: 0.7900\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 0.9419 - acc: 0.8585 - val_loss: 1.0946 - val_acc: 0.7880\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9372 - acc: 0.8593 - val_loss: 1.0889 - val_acc: 0.7850\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9329 - acc: 0.8617 - val_loss: 1.0876 - val_acc: 0.7860\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9287 - acc: 0.8637 - val_loss: 1.0841 - val_acc: 0.7880\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9246 - acc: 0.8619 - val_loss: 1.0828 - val_acc: 0.7850\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9202 - acc: 0.8644 - val_loss: 1.0838 - val_acc: 0.7800\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9162 - acc: 0.8655 - val_loss: 1.0821 - val_acc: 0.7890\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9119 - acc: 0.8671 - val_loss: 1.0752 - val_acc: 0.7870\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9079 - acc: 0.8684 - val_loss: 1.0755 - val_acc: 0.7850\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9041 - acc: 0.8668 - val_loss: 1.0719 - val_acc: 0.7850\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8998 - acc: 0.8703 - val_loss: 1.0703 - val_acc: 0.7810\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8961 - acc: 0.8703 - val_loss: 1.0683 - val_acc: 0.7830\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8920 - acc: 0.8717 - val_loss: 1.0662 - val_acc: 0.7890\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8882 - acc: 0.8732 - val_loss: 1.0617 - val_acc: 0.7880\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8846 - acc: 0.8727 - val_loss: 1.0617 - val_acc: 0.7890\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8805 - acc: 0.8748 - val_loss: 1.0585 - val_acc: 0.7880\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8765 - acc: 0.8756 - val_loss: 1.0615 - val_acc: 0.7860\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8731 - acc: 0.8756 - val_loss: 1.0542 - val_acc: 0.7900\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8695 - acc: 0.8777 - val_loss: 1.0529 - val_acc: 0.7830\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8658 - acc: 0.8785 - val_loss: 1.0512 - val_acc: 0.7870\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8620 - acc: 0.8771 - val_loss: 1.0491 - val_acc: 0.7910\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8586 - acc: 0.8799 - val_loss: 1.0498 - val_acc: 0.7880\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8549 - acc: 0.8803 - val_loss: 1.0482 - val_acc: 0.7870\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8511 - acc: 0.8813 - val_loss: 1.0445 - val_acc: 0.7830\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8478 - acc: 0.8821 - val_loss: 1.0435 - val_acc: 0.7840\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.8445 - acc: 0.8807 - val_loss: 1.0411 - val_acc: 0.7850\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8409 - acc: 0.8835 - val_loss: 1.0414 - val_acc: 0.7880\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8379 - acc: 0.8827 - val_loss: 1.0398 - val_acc: 0.7860\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8343 - acc: 0.8839 - val_loss: 1.0398 - val_acc: 0.7820\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8309 - acc: 0.8839 - val_loss: 1.0351 - val_acc: 0.7870\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, kernel_regularizer=regularizers.l2(0.005), activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'lime', label='Training Acc. L2')\n",
    "plt.plot(epochs, val_acc_values, 'lime', label='Validation Acc. L2')\n",
    "plt.plot(epochs, model_acc, 'midnightblue', label='Training Acc.')\n",
    "plt.plot(epochs, model_val_acc, 'midnightblue', label='Validation Acc.')\n",
    "\n",
    "plt.title('Training & Validation Accuracy L2 vs. regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 2s 260us/step - loss: 15.9971 - acc: 0.1687 - val_loss: 15.5960 - val_acc: 0.1790\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 15.2353 - acc: 0.2047 - val_loss: 14.8518 - val_acc: 0.2040\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 14.4989 - acc: 0.2117 - val_loss: 14.1294 - val_acc: 0.2120\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 13.7843 - acc: 0.2241 - val_loss: 13.4270 - val_acc: 0.2250\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 13.0896 - acc: 0.2335 - val_loss: 12.7438 - val_acc: 0.2370\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 12.4144 - acc: 0.2607 - val_loss: 12.0796 - val_acc: 0.2710\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 11.7589 - acc: 0.2961 - val_loss: 11.4346 - val_acc: 0.3070\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 11.1223 - acc: 0.3353 - val_loss: 10.8081 - val_acc: 0.3590\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 10.5034 - acc: 0.3939 - val_loss: 10.1998 - val_acc: 0.3830\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 9.9031 - acc: 0.4321 - val_loss: 9.6105 - val_acc: 0.4390\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 9.3225 - acc: 0.4832 - val_loss: 9.0432 - val_acc: 0.4550\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 8.7637 - acc: 0.5039 - val_loss: 8.4964 - val_acc: 0.4920\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 8.2267 - acc: 0.5339 - val_loss: 7.9738 - val_acc: 0.5080\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 7.7123 - acc: 0.5540 - val_loss: 7.4709 - val_acc: 0.5400\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 7.2211 - acc: 0.5765 - val_loss: 6.9925 - val_acc: 0.5520\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 6.7520 - acc: 0.5893 - val_loss: 6.5367 - val_acc: 0.5600\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 6.3057 - acc: 0.6063 - val_loss: 6.1021 - val_acc: 0.5770\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 5.8832 - acc: 0.6185 - val_loss: 5.6922 - val_acc: 0.5810\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 5.4847 - acc: 0.6265 - val_loss: 5.3074 - val_acc: 0.5880\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 5.1098 - acc: 0.6305 - val_loss: 4.9453 - val_acc: 0.6110\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 4.7581 - acc: 0.6424 - val_loss: 4.6032 - val_acc: 0.6250\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 4.4291 - acc: 0.6489 - val_loss: 4.2862 - val_acc: 0.6370\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 4.1232 - acc: 0.6548 - val_loss: 3.9921 - val_acc: 0.6490\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 3.8391 - acc: 0.6644 - val_loss: 3.7190 - val_acc: 0.6510\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 3.5781 - acc: 0.6689 - val_loss: 3.4686 - val_acc: 0.6440\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 3.3395 - acc: 0.6707 - val_loss: 3.2419 - val_acc: 0.6460\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 3.1231 - acc: 0.6759 - val_loss: 3.0361 - val_acc: 0.6540\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 2.9283 - acc: 0.6787 - val_loss: 2.8509 - val_acc: 0.6620\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.7543 - acc: 0.6797 - val_loss: 2.6872 - val_acc: 0.6630\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 2.6008 - acc: 0.6800 - val_loss: 2.5430 - val_acc: 0.6720\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 2.4676 - acc: 0.6835 - val_loss: 2.4183 - val_acc: 0.6640\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 2.3545 - acc: 0.6823 - val_loss: 2.3179 - val_acc: 0.6620\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 2.2607 - acc: 0.6828 - val_loss: 2.2304 - val_acc: 0.6650\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 2.1854 - acc: 0.6828 - val_loss: 2.1649 - val_acc: 0.6800\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 2.1270 - acc: 0.6853 - val_loss: 2.1127 - val_acc: 0.6820\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 2.0834 - acc: 0.6855 - val_loss: 2.0763 - val_acc: 0.6810\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.0516 - acc: 0.6843 - val_loss: 2.0455 - val_acc: 0.6790\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 2.0250 - acc: 0.6855 - val_loss: 2.0201 - val_acc: 0.6820\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 2.0019 - acc: 0.6821 - val_loss: 1.9990 - val_acc: 0.6820\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.9812 - acc: 0.6847 - val_loss: 1.9792 - val_acc: 0.6790\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.9619 - acc: 0.6857 - val_loss: 1.9619 - val_acc: 0.6780\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.9443 - acc: 0.6836 - val_loss: 1.9424 - val_acc: 0.6850\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.9272 - acc: 0.6851 - val_loss: 1.9247 - val_acc: 0.6860\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.9113 - acc: 0.6865 - val_loss: 1.9099 - val_acc: 0.6820\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.8959 - acc: 0.6861 - val_loss: 1.8945 - val_acc: 0.6860\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.8813 - acc: 0.6861 - val_loss: 1.8829 - val_acc: 0.6820\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.8673 - acc: 0.6864 - val_loss: 1.8673 - val_acc: 0.6860\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8535 - acc: 0.6864 - val_loss: 1.8529 - val_acc: 0.6810\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.8406 - acc: 0.6880 - val_loss: 1.8429 - val_acc: 0.6880\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.8281 - acc: 0.6892 - val_loss: 1.8296 - val_acc: 0.6820\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8159 - acc: 0.6920 - val_loss: 1.8157 - val_acc: 0.6890\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.8036 - acc: 0.6921 - val_loss: 1.8080 - val_acc: 0.6870\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.7928 - acc: 0.6897 - val_loss: 1.7977 - val_acc: 0.6860\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.7813 - acc: 0.6917 - val_loss: 1.7835 - val_acc: 0.6880\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.7704 - acc: 0.6929 - val_loss: 1.7719 - val_acc: 0.6930\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.7596 - acc: 0.6939 - val_loss: 1.7621 - val_acc: 0.6910\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.7495 - acc: 0.6939 - val_loss: 1.7522 - val_acc: 0.6900\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.7398 - acc: 0.6945 - val_loss: 1.7408 - val_acc: 0.6950\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.7295 - acc: 0.6949 - val_loss: 1.7326 - val_acc: 0.6940\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.7200 - acc: 0.6957 - val_loss: 1.7281 - val_acc: 0.6870\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.7107 - acc: 0.6964 - val_loss: 1.7146 - val_acc: 0.6880\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 1.7014 - acc: 0.6984 - val_loss: 1.7045 - val_acc: 0.7020\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.6923 - acc: 0.6984 - val_loss: 1.6966 - val_acc: 0.6910\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.6834 - acc: 0.6968 - val_loss: 1.6922 - val_acc: 0.6920\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.6747 - acc: 0.6977 - val_loss: 1.6813 - val_acc: 0.7000\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.6662 - acc: 0.6991 - val_loss: 1.6692 - val_acc: 0.6970\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.6578 - acc: 0.6983 - val_loss: 1.6602 - val_acc: 0.6970\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.6495 - acc: 0.7000 - val_loss: 1.6523 - val_acc: 0.6960\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6413 - acc: 0.6996 - val_loss: 1.6448 - val_acc: 0.6940\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.6333 - acc: 0.7004 - val_loss: 1.6376 - val_acc: 0.7040\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6257 - acc: 0.7008 - val_loss: 1.6303 - val_acc: 0.7040\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.6179 - acc: 0.7012 - val_loss: 1.6231 - val_acc: 0.7040\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.6100 - acc: 0.7024 - val_loss: 1.6167 - val_acc: 0.6920\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.6026 - acc: 0.7020 - val_loss: 1.6116 - val_acc: 0.6890\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.5956 - acc: 0.7033 - val_loss: 1.6023 - val_acc: 0.6970\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5884 - acc: 0.7029 - val_loss: 1.5939 - val_acc: 0.6940\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.5807 - acc: 0.7036 - val_loss: 1.5871 - val_acc: 0.7000\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5738 - acc: 0.7037 - val_loss: 1.5795 - val_acc: 0.7020\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.5671 - acc: 0.7025 - val_loss: 1.5751 - val_acc: 0.6930\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.5605 - acc: 0.7055 - val_loss: 1.5686 - val_acc: 0.6930\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.5536 - acc: 0.7064 - val_loss: 1.5598 - val_acc: 0.7000\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.5471 - acc: 0.7057 - val_loss: 1.5532 - val_acc: 0.7000\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5403 - acc: 0.7049 - val_loss: 1.5467 - val_acc: 0.7060\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.5339 - acc: 0.7064 - val_loss: 1.5399 - val_acc: 0.6990\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5275 - acc: 0.7073 - val_loss: 1.5341 - val_acc: 0.7080\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5209 - acc: 0.7083 - val_loss: 1.5291 - val_acc: 0.7080\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5147 - acc: 0.7083 - val_loss: 1.5217 - val_acc: 0.7090\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.5088 - acc: 0.7077 - val_loss: 1.5149 - val_acc: 0.7100\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5022 - acc: 0.7105 - val_loss: 1.5109 - val_acc: 0.7060\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.4963 - acc: 0.7109 - val_loss: 1.5063 - val_acc: 0.7030\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.4906 - acc: 0.7111 - val_loss: 1.5006 - val_acc: 0.7000\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.4845 - acc: 0.7097 - val_loss: 1.4937 - val_acc: 0.7030\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.4784 - acc: 0.7111 - val_loss: 1.4860 - val_acc: 0.7100\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.4722 - acc: 0.7115 - val_loss: 1.4812 - val_acc: 0.7030\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.4666 - acc: 0.7112 - val_loss: 1.4778 - val_acc: 0.7050\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.4607 - acc: 0.7120 - val_loss: 1.4682 - val_acc: 0.7070\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.4550 - acc: 0.7133 - val_loss: 1.4632 - val_acc: 0.7030\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.4496 - acc: 0.7129 - val_loss: 1.4601 - val_acc: 0.7130\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.4438 - acc: 0.7147 - val_loss: 1.4552 - val_acc: 0.7030\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.4383 - acc: 0.7149 - val_loss: 1.4451 - val_acc: 0.7080\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.4328 - acc: 0.7157 - val_loss: 1.4403 - val_acc: 0.7120\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.4271 - acc: 0.7155 - val_loss: 1.4358 - val_acc: 0.7090\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.4217 - acc: 0.7151 - val_loss: 1.4295 - val_acc: 0.7160\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.4162 - acc: 0.7163 - val_loss: 1.4291 - val_acc: 0.7030\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.4117 - acc: 0.7164 - val_loss: 1.4224 - val_acc: 0.7090\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.4059 - acc: 0.7172 - val_loss: 1.4166 - val_acc: 0.7100\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.4013 - acc: 0.7172 - val_loss: 1.4109 - val_acc: 0.7080\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.3955 - acc: 0.7192 - val_loss: 1.4066 - val_acc: 0.7140\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.3905 - acc: 0.7176 - val_loss: 1.4014 - val_acc: 0.7130\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.3855 - acc: 0.7175 - val_loss: 1.3964 - val_acc: 0.7100\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.3803 - acc: 0.7180 - val_loss: 1.3925 - val_acc: 0.7130\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.3753 - acc: 0.7195 - val_loss: 1.3877 - val_acc: 0.7120\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.3705 - acc: 0.7192 - val_loss: 1.3811 - val_acc: 0.7130\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3657 - acc: 0.7200 - val_loss: 1.3819 - val_acc: 0.7110\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.3615 - acc: 0.7212 - val_loss: 1.3711 - val_acc: 0.7110\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3563 - acc: 0.7204 - val_loss: 1.3674 - val_acc: 0.7120\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.3515 - acc: 0.7209 - val_loss: 1.3601 - val_acc: 0.7110\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.3467 - acc: 0.7221 - val_loss: 1.3582 - val_acc: 0.7190\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.3422 - acc: 0.7220 - val_loss: 1.3548 - val_acc: 0.7110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.3379 - acc: 0.7233 - val_loss: 1.3487 - val_acc: 0.7190\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, kernel_regularizer=regularizers.l1(0.005), activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training Acc. L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation Acc. L1')\n",
    "plt.title('Training & Validation Accuracy with L1 Regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 15.9977 - acc: 0.1657 - val_loss: 15.5959 - val_acc: 0.1710\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 15.2401 - acc: 0.1969 - val_loss: 14.8540 - val_acc: 0.2050\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 14.5072 - acc: 0.2205 - val_loss: 14.1339 - val_acc: 0.2220\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 13.7954 - acc: 0.2440 - val_loss: 13.4338 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 13.1027 - acc: 0.2623 - val_loss: 12.7514 - val_acc: 0.2630\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 12.4272 - acc: 0.2853 - val_loss: 12.0858 - val_acc: 0.3000\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 11.7693 - acc: 0.3231 - val_loss: 11.4393 - val_acc: 0.3170\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 11.1292 - acc: 0.3488 - val_loss: 10.8087 - val_acc: 0.3410\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 10.5071 - acc: 0.3804 - val_loss: 10.1967 - val_acc: 0.3920\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 9.9044 - acc: 0.4033 - val_loss: 9.6044 - val_acc: 0.4170\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 9.3223 - acc: 0.4292 - val_loss: 9.0337 - val_acc: 0.4340\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 8.7623 - acc: 0.4464 - val_loss: 8.4864 - val_acc: 0.4610\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 8.2256 - acc: 0.4695 - val_loss: 7.9623 - val_acc: 0.4750\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 7.7119 - acc: 0.4815 - val_loss: 7.4614 - val_acc: 0.5060\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 7.2207 - acc: 0.5065 - val_loss: 6.9811 - val_acc: 0.5210\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 6.7522 - acc: 0.5201 - val_loss: 6.5255 - val_acc: 0.5330\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 6.3077 - acc: 0.5353 - val_loss: 6.0962 - val_acc: 0.5340\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 5.8871 - acc: 0.5504 - val_loss: 5.6874 - val_acc: 0.5660\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 5.4905 - acc: 0.5680 - val_loss: 5.3032 - val_acc: 0.5810\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 5.1170 - acc: 0.5793 - val_loss: 4.9413 - val_acc: 0.5910\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 4.7658 - acc: 0.5923 - val_loss: 4.6022 - val_acc: 0.5980\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 4.4374 - acc: 0.5976 - val_loss: 4.2867 - val_acc: 0.5900\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 4.1301 - acc: 0.6081 - val_loss: 3.9896 - val_acc: 0.6130\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 3.8451 - acc: 0.6144 - val_loss: 3.7164 - val_acc: 0.6210\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 3.5828 - acc: 0.6235 - val_loss: 3.4656 - val_acc: 0.6350\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 3.3429 - acc: 0.6337 - val_loss: 3.2368 - val_acc: 0.6380\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 3.1247 - acc: 0.6333 - val_loss: 3.0291 - val_acc: 0.6440\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 2.9283 - acc: 0.6429 - val_loss: 2.8460 - val_acc: 0.6480\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 2.7535 - acc: 0.6477 - val_loss: 2.6809 - val_acc: 0.6470\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 2.6001 - acc: 0.6493 - val_loss: 2.5371 - val_acc: 0.6560\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 2.4667 - acc: 0.6529 - val_loss: 2.4145 - val_acc: 0.6610\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 2.3534 - acc: 0.6579 - val_loss: 2.3087 - val_acc: 0.6750\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 2.2593 - acc: 0.6616 - val_loss: 2.2230 - val_acc: 0.6730\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 2.1838 - acc: 0.6607 - val_loss: 2.1565 - val_acc: 0.6790\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 2.1252 - acc: 0.6617 - val_loss: 2.1051 - val_acc: 0.6670\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 2.0813 - acc: 0.6595 - val_loss: 2.0674 - val_acc: 0.6740\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 2.0493 - acc: 0.6612 - val_loss: 2.0379 - val_acc: 0.6740\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 2.0232 - acc: 0.6620 - val_loss: 2.0148 - val_acc: 0.6760\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 2.0015 - acc: 0.6664 - val_loss: 1.9944 - val_acc: 0.6810\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.9816 - acc: 0.6668 - val_loss: 1.9744 - val_acc: 0.6790\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.9633 - acc: 0.6700 - val_loss: 1.9581 - val_acc: 0.6750\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 1.9462 - acc: 0.6716 - val_loss: 1.9444 - val_acc: 0.6730\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.9304 - acc: 0.6724 - val_loss: 1.9272 - val_acc: 0.6730\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.9151 - acc: 0.6729 - val_loss: 1.9074 - val_acc: 0.6810\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.8999 - acc: 0.6748 - val_loss: 1.8971 - val_acc: 0.6810\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.8856 - acc: 0.6772 - val_loss: 1.8808 - val_acc: 0.6800\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.8725 - acc: 0.6751 - val_loss: 1.8681 - val_acc: 0.6870\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.8595 - acc: 0.6776 - val_loss: 1.8537 - val_acc: 0.6930\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.8471 - acc: 0.6787 - val_loss: 1.8417 - val_acc: 0.6900\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.8348 - acc: 0.6804 - val_loss: 1.8300 - val_acc: 0.6940\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 1.8232 - acc: 0.6789 - val_loss: 1.8191 - val_acc: 0.6950\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.8118 - acc: 0.6828 - val_loss: 1.8038 - val_acc: 0.6960\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.8005 - acc: 0.6813 - val_loss: 1.7948 - val_acc: 0.6940\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.7899 - acc: 0.6825 - val_loss: 1.7834 - val_acc: 0.6910\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 1.7792 - acc: 0.6841 - val_loss: 1.7727 - val_acc: 0.6950\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.7684 - acc: 0.6840 - val_loss: 1.7626 - val_acc: 0.6950\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.7589 - acc: 0.6847 - val_loss: 1.7545 - val_acc: 0.6910\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.7493 - acc: 0.6847 - val_loss: 1.7437 - val_acc: 0.6990\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.7393 - acc: 0.6869 - val_loss: 1.7333 - val_acc: 0.6990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.7301 - acc: 0.6872 - val_loss: 1.7244 - val_acc: 0.6980\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.7208 - acc: 0.6884 - val_loss: 1.7132 - val_acc: 0.6960\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.7115 - acc: 0.6887 - val_loss: 1.7057 - val_acc: 0.7000\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.7022 - acc: 0.6891 - val_loss: 1.6959 - val_acc: 0.7000\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.6938 - acc: 0.6904 - val_loss: 1.6863 - val_acc: 0.7020\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.6850 - acc: 0.6887 - val_loss: 1.6842 - val_acc: 0.7000\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.6765 - acc: 0.6892 - val_loss: 1.6695 - val_acc: 0.6990\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.6676 - acc: 0.6897 - val_loss: 1.6635 - val_acc: 0.6970\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.6596 - acc: 0.6933 - val_loss: 1.6521 - val_acc: 0.7010\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.6513 - acc: 0.6905 - val_loss: 1.6465 - val_acc: 0.7030\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.6435 - acc: 0.6923 - val_loss: 1.6377 - val_acc: 0.7000\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.6349 - acc: 0.6931 - val_loss: 1.6301 - val_acc: 0.7050\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.6275 - acc: 0.6939 - val_loss: 1.6254 - val_acc: 0.7050\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.6197 - acc: 0.6933 - val_loss: 1.6162 - val_acc: 0.7010\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.6119 - acc: 0.6952 - val_loss: 1.6073 - val_acc: 0.7040\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.6043 - acc: 0.6953 - val_loss: 1.5995 - val_acc: 0.7040\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.5964 - acc: 0.6965 - val_loss: 1.5904 - val_acc: 0.7020\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.5891 - acc: 0.6957 - val_loss: 1.5846 - val_acc: 0.7000\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5816 - acc: 0.6972 - val_loss: 1.5804 - val_acc: 0.7010\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.5747 - acc: 0.6967 - val_loss: 1.5716 - val_acc: 0.7050\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5679 - acc: 0.6972 - val_loss: 1.5682 - val_acc: 0.7000\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.5613 - acc: 0.6965 - val_loss: 1.5617 - val_acc: 0.7030\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5538 - acc: 0.6997 - val_loss: 1.5497 - val_acc: 0.7050\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.5466 - acc: 0.6984 - val_loss: 1.5460 - val_acc: 0.7060\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.5395 - acc: 0.6989 - val_loss: 1.5367 - val_acc: 0.7060\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.5329 - acc: 0.7012 - val_loss: 1.5306 - val_acc: 0.7030\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.5264 - acc: 0.6995 - val_loss: 1.5249 - val_acc: 0.7090\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5194 - acc: 0.7008 - val_loss: 1.5156 - val_acc: 0.7050\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.5130 - acc: 0.7012 - val_loss: 1.5073 - val_acc: 0.7060\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5062 - acc: 0.7011 - val_loss: 1.5018 - val_acc: 0.7070\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.5000 - acc: 0.7016 - val_loss: 1.4960 - val_acc: 0.7060\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.4941 - acc: 0.7023 - val_loss: 1.4879 - val_acc: 0.7090\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.4871 - acc: 0.7023 - val_loss: 1.4840 - val_acc: 0.7070\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.4807 - acc: 0.7021 - val_loss: 1.4786 - val_acc: 0.7130\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.4748 - acc: 0.7023 - val_loss: 1.4704 - val_acc: 0.7120\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.4681 - acc: 0.7048 - val_loss: 1.4671 - val_acc: 0.7140\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.4626 - acc: 0.7037 - val_loss: 1.4600 - val_acc: 0.7100\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.4572 - acc: 0.7040 - val_loss: 1.4516 - val_acc: 0.7110\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.4507 - acc: 0.7040 - val_loss: 1.4505 - val_acc: 0.7080\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.4450 - acc: 0.7049 - val_loss: 1.4433 - val_acc: 0.7110\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.4391 - acc: 0.7048 - val_loss: 1.4353 - val_acc: 0.7110\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 1.4331 - acc: 0.7069 - val_loss: 1.4337 - val_acc: 0.7120\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.4279 - acc: 0.7051 - val_loss: 1.4244 - val_acc: 0.7150\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.4223 - acc: 0.7065 - val_loss: 1.4168 - val_acc: 0.7100\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.4161 - acc: 0.7067 - val_loss: 1.4171 - val_acc: 0.7190\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.4109 - acc: 0.7067 - val_loss: 1.4081 - val_acc: 0.7180\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.4058 - acc: 0.7077 - val_loss: 1.4035 - val_acc: 0.7160\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.4002 - acc: 0.7071 - val_loss: 1.3971 - val_acc: 0.7180\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.3947 - acc: 0.7091 - val_loss: 1.3951 - val_acc: 0.7210\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3898 - acc: 0.7096 - val_loss: 1.3900 - val_acc: 0.7190\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.3843 - acc: 0.7100 - val_loss: 1.3802 - val_acc: 0.7190\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.3786 - acc: 0.7095 - val_loss: 1.3798 - val_acc: 0.7170\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.3738 - acc: 0.7104 - val_loss: 1.3729 - val_acc: 0.7200\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.3690 - acc: 0.7101 - val_loss: 1.3648 - val_acc: 0.7190\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3634 - acc: 0.7125 - val_loss: 1.3598 - val_acc: 0.7190\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3580 - acc: 0.7120 - val_loss: 1.3599 - val_acc: 0.7220\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.3532 - acc: 0.7117 - val_loss: 1.3520 - val_acc: 0.7240\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.3482 - acc: 0.7119 - val_loss: 1.3480 - val_acc: 0.7140\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.3429 - acc: 0.7139 - val_loss: 1.3388 - val_acc: 0.7220\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.3376 - acc: 0.7144 - val_loss: 1.3349 - val_acc: 0.7250\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3336 - acc: 0.7153 - val_loss: 1.3308 - val_acc: 0.7220\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.3278 - acc: 0.7145 - val_loss: 1.3279 - val_acc: 0.7220\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.3228 - acc: 0.7159 - val_loss: 1.3221 - val_acc: 0.7220\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.3193 - acc: 0.7155 - val_loss: 1.3160 - val_acc: 0.7250\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.3138 - acc: 0.7167 - val_loss: 1.3133 - val_acc: 0.7250\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.3099 - acc: 0.7149 - val_loss: 1.3070 - val_acc: 0.7270\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.3049 - acc: 0.7159 - val_loss: 1.3026 - val_acc: 0.7270\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.3004 - acc: 0.7161 - val_loss: 1.2985 - val_acc: 0.7240\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.2959 - acc: 0.7163 - val_loss: 1.2933 - val_acc: 0.7280\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.2916 - acc: 0.7164 - val_loss: 1.2949 - val_acc: 0.7230\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.2867 - acc: 0.7179 - val_loss: 1.2868 - val_acc: 0.7230\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.2824 - acc: 0.7185 - val_loss: 1.2841 - val_acc: 0.7260\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.2787 - acc: 0.7189 - val_loss: 1.2775 - val_acc: 0.7250\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.2746 - acc: 0.7181 - val_loss: 1.2736 - val_acc: 0.7260\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.2699 - acc: 0.7180 - val_loss: 1.2708 - val_acc: 0.7250\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.2654 - acc: 0.7203 - val_loss: 1.2656 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.2616 - acc: 0.7208 - val_loss: 1.2615 - val_acc: 0.7290\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.2576 - acc: 0.7195 - val_loss: 1.2583 - val_acc: 0.7300\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.2533 - acc: 0.7201 - val_loss: 1.2556 - val_acc: 0.7270\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.2491 - acc: 0.7223 - val_loss: 1.2487 - val_acc: 0.7270\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.2453 - acc: 0.7203 - val_loss: 1.2452 - val_acc: 0.7280\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.2419 - acc: 0.7237 - val_loss: 1.2441 - val_acc: 0.7270\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2377 - acc: 0.7220 - val_loss: 1.2365 - val_acc: 0.7300\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.2341 - acc: 0.7211 - val_loss: 1.2361 - val_acc: 0.7260\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.2301 - acc: 0.7215 - val_loss: 1.2307 - val_acc: 0.7310\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.2268 - acc: 0.7221 - val_loss: 1.2238 - val_acc: 0.7290\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.2225 - acc: 0.7225 - val_loss: 1.2243 - val_acc: 0.7290\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.2193 - acc: 0.7225 - val_loss: 1.2191 - val_acc: 0.7320\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.2155 - acc: 0.7220 - val_loss: 1.2171 - val_acc: 0.7250\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.2124 - acc: 0.7240 - val_loss: 1.2141 - val_acc: 0.7290\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.2082 - acc: 0.7244 - val_loss: 1.2087 - val_acc: 0.7250\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.2053 - acc: 0.7247 - val_loss: 1.2053 - val_acc: 0.7320\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.2023 - acc: 0.7241 - val_loss: 1.2039 - val_acc: 0.7260\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.1986 - acc: 0.7252 - val_loss: 1.1989 - val_acc: 0.7290\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.1950 - acc: 0.7263 - val_loss: 1.1970 - val_acc: 0.7260\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 1.1923 - acc: 0.7240 - val_loss: 1.1933 - val_acc: 0.7280\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.1891 - acc: 0.7249 - val_loss: 1.1944 - val_acc: 0.7270\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1856 - acc: 0.7279 - val_loss: 1.1860 - val_acc: 0.7290\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.1822 - acc: 0.7271 - val_loss: 1.1865 - val_acc: 0.7290\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 1.1798 - acc: 0.7259 - val_loss: 1.1816 - val_acc: 0.7320\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 1.1769 - acc: 0.7263 - val_loss: 1.1869 - val_acc: 0.7240\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1737 - acc: 0.7263 - val_loss: 1.1764 - val_acc: 0.7290\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.1702 - acc: 0.7276 - val_loss: 1.1751 - val_acc: 0.7310\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1677 - acc: 0.7284 - val_loss: 1.1718 - val_acc: 0.7320\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.1650 - acc: 0.7285 - val_loss: 1.1708 - val_acc: 0.7290\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 1.1614 - acc: 0.7292 - val_loss: 1.1685 - val_acc: 0.7280\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.1591 - acc: 0.7295 - val_loss: 1.1677 - val_acc: 0.7310\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.1561 - acc: 0.7296 - val_loss: 1.1641 - val_acc: 0.7310\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1539 - acc: 0.7305 - val_loss: 1.1566 - val_acc: 0.7300\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.1506 - acc: 0.7291 - val_loss: 1.1548 - val_acc: 0.7370\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 1.1483 - acc: 0.7303 - val_loss: 1.1511 - val_acc: 0.7320\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 1.1454 - acc: 0.7316 - val_loss: 1.1492 - val_acc: 0.7280\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 1.1432 - acc: 0.7308 - val_loss: 1.1480 - val_acc: 0.7280\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 1.1405 - acc: 0.7313 - val_loss: 1.1495 - val_acc: 0.7300\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.1380 - acc: 0.7320 - val_loss: 1.1457 - val_acc: 0.7350\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 1.1356 - acc: 0.7321 - val_loss: 1.1431 - val_acc: 0.7330\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.1338 - acc: 0.7309 - val_loss: 1.1387 - val_acc: 0.7310\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 1.1308 - acc: 0.7319 - val_loss: 1.1361 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 1.1286 - acc: 0.7325 - val_loss: 1.1313 - val_acc: 0.7290\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.1264 - acc: 0.7328 - val_loss: 1.1319 - val_acc: 0.7330\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 1.1245 - acc: 0.7327 - val_loss: 1.1320 - val_acc: 0.7260\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.1219 - acc: 0.7323 - val_loss: 1.1282 - val_acc: 0.7300\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 1.1196 - acc: 0.7348 - val_loss: 1.1249 - val_acc: 0.7300\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1173 - acc: 0.7345 - val_loss: 1.1250 - val_acc: 0.7310\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.1166 - acc: 0.7345 - val_loss: 1.1207 - val_acc: 0.7340\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.1141 - acc: 0.7344 - val_loss: 1.1197 - val_acc: 0.7310\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.1121 - acc: 0.7331 - val_loss: 1.1178 - val_acc: 0.7350\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1104 - acc: 0.7343 - val_loss: 1.1157 - val_acc: 0.7320\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1080 - acc: 0.7368 - val_loss: 1.1115 - val_acc: 0.7340\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 1.1059 - acc: 0.7365 - val_loss: 1.1159 - val_acc: 0.7320\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.1039 - acc: 0.7353 - val_loss: 1.1123 - val_acc: 0.7320\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.1027 - acc: 0.7369 - val_loss: 1.1124 - val_acc: 0.7340\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.1009 - acc: 0.7364 - val_loss: 1.1114 - val_acc: 0.7310\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.0990 - acc: 0.7361 - val_loss: 1.1040 - val_acc: 0.7360\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 1.0969 - acc: 0.7357 - val_loss: 1.1069 - val_acc: 0.7340\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.0956 - acc: 0.7381 - val_loss: 1.0999 - val_acc: 0.7290\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 1.0925 - acc: 0.7363 - val_loss: 1.1005 - val_acc: 0.7330\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 1.0915 - acc: 0.7377 - val_loss: 1.1068 - val_acc: 0.7320\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.0907 - acc: 0.7368 - val_loss: 1.0946 - val_acc: 0.7360\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0888 - acc: 0.7371 - val_loss: 1.0954 - val_acc: 0.7370\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.0876 - acc: 0.7383 - val_loss: 1.0929 - val_acc: 0.7310\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0855 - acc: 0.7380 - val_loss: 1.0910 - val_acc: 0.7410\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 1.0839 - acc: 0.7373 - val_loss: 1.0891 - val_acc: 0.7360\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0821 - acc: 0.7380 - val_loss: 1.0895 - val_acc: 0.7450\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.0812 - acc: 0.7403 - val_loss: 1.0887 - val_acc: 0.7370\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0799 - acc: 0.7393 - val_loss: 1.1043 - val_acc: 0.7310\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0798 - acc: 0.7393 - val_loss: 1.0857 - val_acc: 0.7380\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0762 - acc: 0.7395 - val_loss: 1.0820 - val_acc: 0.7360\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 1.0748 - acc: 0.7403 - val_loss: 1.0882 - val_acc: 0.7330\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 1.0732 - acc: 0.7401 - val_loss: 1.0802 - val_acc: 0.7380\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0721 - acc: 0.7395 - val_loss: 1.0843 - val_acc: 0.7370\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.0707 - acc: 0.7381 - val_loss: 1.0797 - val_acc: 0.7360\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0691 - acc: 0.7405 - val_loss: 1.0816 - val_acc: 0.7360\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0684 - acc: 0.7393 - val_loss: 1.0733 - val_acc: 0.7380\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.0664 - acc: 0.7431 - val_loss: 1.0780 - val_acc: 0.7380\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0658 - acc: 0.7391 - val_loss: 1.0744 - val_acc: 0.7420\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.0634 - acc: 0.7428 - val_loss: 1.0725 - val_acc: 0.7370\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0629 - acc: 0.7413 - val_loss: 1.0729 - val_acc: 0.7360\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0620 - acc: 0.7411 - val_loss: 1.0685 - val_acc: 0.7410\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0591 - acc: 0.7416 - val_loss: 1.0707 - val_acc: 0.7380\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.0576 - acc: 0.7440 - val_loss: 1.0854 - val_acc: 0.7390\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0577 - acc: 0.7417 - val_loss: 1.0673 - val_acc: 0.7380\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0552 - acc: 0.7436 - val_loss: 1.0668 - val_acc: 0.7400\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0545 - acc: 0.7437 - val_loss: 1.0674 - val_acc: 0.7390\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0528 - acc: 0.7437 - val_loss: 1.0624 - val_acc: 0.7400\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0517 - acc: 0.7431 - val_loss: 1.0618 - val_acc: 0.7400\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.0514 - acc: 0.7432 - val_loss: 1.0706 - val_acc: 0.7410\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0498 - acc: 0.7419 - val_loss: 1.0572 - val_acc: 0.7400\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 1.0485 - acc: 0.7432 - val_loss: 1.0569 - val_acc: 0.7400\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0473 - acc: 0.7440 - val_loss: 1.0556 - val_acc: 0.7450\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.0459 - acc: 0.7452 - val_loss: 1.0708 - val_acc: 0.7350\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0453 - acc: 0.7441 - val_loss: 1.0574 - val_acc: 0.7380\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 1.0447 - acc: 0.7432 - val_loss: 1.0529 - val_acc: 0.7420\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.0430 - acc: 0.7456 - val_loss: 1.0529 - val_acc: 0.7400\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.0416 - acc: 0.7453 - val_loss: 1.0537 - val_acc: 0.7410\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 1.0398 - acc: 0.7453 - val_loss: 1.0508 - val_acc: 0.7430\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0398 - acc: 0.7448 - val_loss: 1.0581 - val_acc: 0.7400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0380 - acc: 0.7451 - val_loss: 1.0490 - val_acc: 0.7430\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.0367 - acc: 0.7441 - val_loss: 1.0530 - val_acc: 0.7430\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0371 - acc: 0.7440 - val_loss: 1.0470 - val_acc: 0.7430\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0348 - acc: 0.7463 - val_loss: 1.0441 - val_acc: 0.7440\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0333 - acc: 0.7451 - val_loss: 1.0483 - val_acc: 0.7410\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0328 - acc: 0.7449 - val_loss: 1.0493 - val_acc: 0.7430\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0327 - acc: 0.7459 - val_loss: 1.0427 - val_acc: 0.7440\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0308 - acc: 0.7452 - val_loss: 1.0497 - val_acc: 0.7420\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0302 - acc: 0.7448 - val_loss: 1.0408 - val_acc: 0.7420\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0282 - acc: 0.7447 - val_loss: 1.0383 - val_acc: 0.7440\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.0278 - acc: 0.7476 - val_loss: 1.0522 - val_acc: 0.7430\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0279 - acc: 0.7456 - val_loss: 1.0418 - val_acc: 0.7430\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0261 - acc: 0.7471 - val_loss: 1.0404 - val_acc: 0.7400\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0247 - acc: 0.7461 - val_loss: 1.0371 - val_acc: 0.7450\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0254 - acc: 0.7464 - val_loss: 1.0345 - val_acc: 0.7450\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0228 - acc: 0.7469 - val_loss: 1.0377 - val_acc: 0.7460\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0222 - acc: 0.7465 - val_loss: 1.0361 - val_acc: 0.7420\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0207 - acc: 0.7497 - val_loss: 1.0387 - val_acc: 0.7430\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 1.0212 - acc: 0.7472 - val_loss: 1.0330 - val_acc: 0.7450\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.0197 - acc: 0.7483 - val_loss: 1.0341 - val_acc: 0.7460\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.0191 - acc: 0.7487 - val_loss: 1.0296 - val_acc: 0.7440\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0178 - acc: 0.7479 - val_loss: 1.0459 - val_acc: 0.7470\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0181 - acc: 0.7489 - val_loss: 1.0382 - val_acc: 0.7430\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0170 - acc: 0.7495 - val_loss: 1.0358 - val_acc: 0.7480\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0166 - acc: 0.7485 - val_loss: 1.0266 - val_acc: 0.7450\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0151 - acc: 0.7487 - val_loss: 1.0302 - val_acc: 0.7470\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0150 - acc: 0.7492 - val_loss: 1.0343 - val_acc: 0.7460\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0134 - acc: 0.7477 - val_loss: 1.0251 - val_acc: 0.7430\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0123 - acc: 0.7483 - val_loss: 1.0234 - val_acc: 0.7430\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.0111 - acc: 0.7504 - val_loss: 1.0269 - val_acc: 0.7440\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.0108 - acc: 0.7501 - val_loss: 1.0275 - val_acc: 0.7470\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0103 - acc: 0.7500 - val_loss: 1.0301 - val_acc: 0.7490\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0098 - acc: 0.7479 - val_loss: 1.0232 - val_acc: 0.7460\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.0079 - acc: 0.7519 - val_loss: 1.0308 - val_acc: 0.7420\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.0086 - acc: 0.7507 - val_loss: 1.0222 - val_acc: 0.7460\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0058 - acc: 0.7507 - val_loss: 1.0209 - val_acc: 0.7460\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0059 - acc: 0.7508 - val_loss: 1.0229 - val_acc: 0.7500\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0050 - acc: 0.7513 - val_loss: 1.0181 - val_acc: 0.7500\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0036 - acc: 0.7504 - val_loss: 1.0345 - val_acc: 0.7450\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0038 - acc: 0.7516 - val_loss: 1.0164 - val_acc: 0.7480\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.0026 - acc: 0.7513 - val_loss: 1.0193 - val_acc: 0.7450\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.0018 - acc: 0.7521 - val_loss: 1.0183 - val_acc: 0.7430\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0017 - acc: 0.7524 - val_loss: 1.0203 - val_acc: 0.7440\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0011 - acc: 0.7536 - val_loss: 1.0162 - val_acc: 0.7510\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9996 - acc: 0.7528 - val_loss: 1.0180 - val_acc: 0.7510\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0003 - acc: 0.7505 - val_loss: 1.0248 - val_acc: 0.7470\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9987 - acc: 0.7520 - val_loss: 1.0128 - val_acc: 0.7510\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9972 - acc: 0.7509 - val_loss: 1.0140 - val_acc: 0.7470\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9968 - acc: 0.7525 - val_loss: 1.0118 - val_acc: 0.7440\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9958 - acc: 0.7507 - val_loss: 1.0152 - val_acc: 0.7490\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9959 - acc: 0.7536 - val_loss: 1.0115 - val_acc: 0.7500\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.9942 - acc: 0.7552 - val_loss: 1.0199 - val_acc: 0.7430\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9941 - acc: 0.7519 - val_loss: 1.0121 - val_acc: 0.7490\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9933 - acc: 0.7508 - val_loss: 1.0072 - val_acc: 0.7470\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9917 - acc: 0.7536 - val_loss: 1.0160 - val_acc: 0.7420\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9920 - acc: 0.7529 - val_loss: 1.0071 - val_acc: 0.7480\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9919 - acc: 0.7529 - val_loss: 1.0086 - val_acc: 0.7460\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9901 - acc: 0.7553 - val_loss: 1.0113 - val_acc: 0.7490\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9910 - acc: 0.7527 - val_loss: 1.0309 - val_acc: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9913 - acc: 0.7517 - val_loss: 1.0063 - val_acc: 0.7490\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9889 - acc: 0.7533 - val_loss: 1.0036 - val_acc: 0.7490\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9872 - acc: 0.7541 - val_loss: 1.0051 - val_acc: 0.7450\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9881 - acc: 0.7540 - val_loss: 1.0085 - val_acc: 0.7550\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9870 - acc: 0.7520 - val_loss: 1.0018 - val_acc: 0.7510\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9858 - acc: 0.7552 - val_loss: 1.0058 - val_acc: 0.7420\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9854 - acc: 0.7547 - val_loss: 1.0021 - val_acc: 0.7500\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9848 - acc: 0.7547 - val_loss: 1.0033 - val_acc: 0.7490\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9839 - acc: 0.7528 - val_loss: 1.0014 - val_acc: 0.7520\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9829 - acc: 0.7549 - val_loss: 1.0066 - val_acc: 0.7500\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9827 - acc: 0.7541 - val_loss: 0.9977 - val_acc: 0.7510\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9823 - acc: 0.7553 - val_loss: 0.9988 - val_acc: 0.7470\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9813 - acc: 0.7564 - val_loss: 0.9979 - val_acc: 0.7510\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9809 - acc: 0.7535 - val_loss: 1.0029 - val_acc: 0.7470\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9799 - acc: 0.7540 - val_loss: 1.0031 - val_acc: 0.7480\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9809 - acc: 0.7556 - val_loss: 0.9958 - val_acc: 0.7510\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9788 - acc: 0.7557 - val_loss: 1.0032 - val_acc: 0.7490\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9782 - acc: 0.7560 - val_loss: 1.0033 - val_acc: 0.7470\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9783 - acc: 0.7551 - val_loss: 0.9956 - val_acc: 0.7510\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9765 - acc: 0.7556 - val_loss: 0.9997 - val_acc: 0.7490\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9767 - acc: 0.7560 - val_loss: 0.9977 - val_acc: 0.7450\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9761 - acc: 0.7555 - val_loss: 1.0092 - val_acc: 0.7460\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9758 - acc: 0.7543 - val_loss: 0.9962 - val_acc: 0.7470\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9745 - acc: 0.7565 - val_loss: 0.9934 - val_acc: 0.7530\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9743 - acc: 0.7563 - val_loss: 0.9945 - val_acc: 0.7470\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9734 - acc: 0.7567 - val_loss: 1.0070 - val_acc: 0.7510\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9734 - acc: 0.7573 - val_loss: 0.9918 - val_acc: 0.7550\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9728 - acc: 0.7571 - val_loss: 0.9902 - val_acc: 0.7500\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9715 - acc: 0.7553 - val_loss: 0.9944 - val_acc: 0.7480\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9709 - acc: 0.7581 - val_loss: 0.9955 - val_acc: 0.7520\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9704 - acc: 0.7573 - val_loss: 0.9952 - val_acc: 0.7500\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9698 - acc: 0.7576 - val_loss: 0.9939 - val_acc: 0.7440\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9700 - acc: 0.7588 - val_loss: 0.9926 - val_acc: 0.7480\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9692 - acc: 0.7580 - val_loss: 0.9962 - val_acc: 0.7500\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9687 - acc: 0.7580 - val_loss: 0.9914 - val_acc: 0.7490\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9689 - acc: 0.7561 - val_loss: 0.9997 - val_acc: 0.7510\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9682 - acc: 0.7571 - val_loss: 0.9903 - val_acc: 0.7540\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9672 - acc: 0.7591 - val_loss: 0.9917 - val_acc: 0.7470\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9669 - acc: 0.7569 - val_loss: 0.9871 - val_acc: 0.7460\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9660 - acc: 0.7601 - val_loss: 0.9922 - val_acc: 0.7500\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9659 - acc: 0.7587 - val_loss: 0.9851 - val_acc: 0.7520\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9645 - acc: 0.7581 - val_loss: 0.9900 - val_acc: 0.7490\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9645 - acc: 0.7591 - val_loss: 0.9888 - val_acc: 0.7450\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9633 - acc: 0.7592 - val_loss: 0.9847 - val_acc: 0.7530\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9636 - acc: 0.7612 - val_loss: 0.9904 - val_acc: 0.7490\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9627 - acc: 0.7587 - val_loss: 0.9827 - val_acc: 0.7520\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9625 - acc: 0.7593 - val_loss: 0.9931 - val_acc: 0.7550\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9614 - acc: 0.7601 - val_loss: 0.9872 - val_acc: 0.7530\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9605 - acc: 0.7603 - val_loss: 0.9818 - val_acc: 0.7520\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9596 - acc: 0.7599 - val_loss: 0.9811 - val_acc: 0.7530\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9599 - acc: 0.7589 - val_loss: 0.9886 - val_acc: 0.7490\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9596 - acc: 0.7592 - val_loss: 0.9918 - val_acc: 0.7570\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9592 - acc: 0.7588 - val_loss: 0.9842 - val_acc: 0.7520\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9584 - acc: 0.7600 - val_loss: 0.9819 - val_acc: 0.7520\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9583 - acc: 0.7584 - val_loss: 0.9858 - val_acc: 0.7490\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9582 - acc: 0.7591 - val_loss: 0.9924 - val_acc: 0.7510\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9563 - acc: 0.7613 - val_loss: 0.9819 - val_acc: 0.7470\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9555 - acc: 0.7612 - val_loss: 0.9821 - val_acc: 0.7470\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9552 - acc: 0.7612 - val_loss: 0.9814 - val_acc: 0.7580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9561 - acc: 0.7604 - val_loss: 0.9763 - val_acc: 0.7500\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9549 - acc: 0.7620 - val_loss: 0.9874 - val_acc: 0.7500\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9539 - acc: 0.7615 - val_loss: 0.9871 - val_acc: 0.7490\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9532 - acc: 0.7620 - val_loss: 0.9934 - val_acc: 0.7570\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9540 - acc: 0.7601 - val_loss: 0.9761 - val_acc: 0.7510\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9528 - acc: 0.7600 - val_loss: 0.9785 - val_acc: 0.7540\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9523 - acc: 0.7620 - val_loss: 0.9761 - val_acc: 0.7550\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9514 - acc: 0.7623 - val_loss: 0.9770 - val_acc: 0.7560\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9522 - acc: 0.7601 - val_loss: 0.9808 - val_acc: 0.7440\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9515 - acc: 0.7617 - val_loss: 0.9746 - val_acc: 0.7510\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9499 - acc: 0.7629 - val_loss: 0.9818 - val_acc: 0.7600\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9505 - acc: 0.7615 - val_loss: 0.9745 - val_acc: 0.7530\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9493 - acc: 0.7641 - val_loss: 0.9805 - val_acc: 0.7600\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9496 - acc: 0.7611 - val_loss: 0.9814 - val_acc: 0.7530\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9479 - acc: 0.7621 - val_loss: 0.9748 - val_acc: 0.7480\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9476 - acc: 0.7624 - val_loss: 0.9716 - val_acc: 0.7500\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9477 - acc: 0.7627 - val_loss: 0.9748 - val_acc: 0.7550\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.9462 - acc: 0.7616 - val_loss: 0.9731 - val_acc: 0.7520\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9455 - acc: 0.7627 - val_loss: 0.9722 - val_acc: 0.7510\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9457 - acc: 0.7624 - val_loss: 0.9780 - val_acc: 0.7530\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9453 - acc: 0.7644 - val_loss: 0.9729 - val_acc: 0.7560\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9452 - acc: 0.7636 - val_loss: 0.9728 - val_acc: 0.7530\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9450 - acc: 0.7616 - val_loss: 0.9726 - val_acc: 0.7600\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9438 - acc: 0.7627 - val_loss: 0.9974 - val_acc: 0.7540\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9446 - acc: 0.7624 - val_loss: 0.9702 - val_acc: 0.7580\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9437 - acc: 0.7623 - val_loss: 0.9694 - val_acc: 0.7560\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9422 - acc: 0.7623 - val_loss: 0.9943 - val_acc: 0.7540\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9424 - acc: 0.7644 - val_loss: 0.9805 - val_acc: 0.7540\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9420 - acc: 0.7628 - val_loss: 0.9784 - val_acc: 0.7530\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9429 - acc: 0.7623 - val_loss: 0.9759 - val_acc: 0.7520\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9417 - acc: 0.7629 - val_loss: 0.9670 - val_acc: 0.7520\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9403 - acc: 0.7641 - val_loss: 0.9656 - val_acc: 0.7550\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9401 - acc: 0.7632 - val_loss: 0.9780 - val_acc: 0.7560\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9391 - acc: 0.7637 - val_loss: 0.9696 - val_acc: 0.7520\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9394 - acc: 0.7640 - val_loss: 0.9691 - val_acc: 0.7520\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9390 - acc: 0.7644 - val_loss: 0.9707 - val_acc: 0.7550\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9383 - acc: 0.7636 - val_loss: 0.9844 - val_acc: 0.7550\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9382 - acc: 0.7645 - val_loss: 0.9736 - val_acc: 0.7520\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9375 - acc: 0.7629 - val_loss: 0.9662 - val_acc: 0.7530\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9363 - acc: 0.7652 - val_loss: 0.9696 - val_acc: 0.7490\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9361 - acc: 0.7656 - val_loss: 0.9704 - val_acc: 0.7500\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9359 - acc: 0.7643 - val_loss: 0.9702 - val_acc: 0.7540\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9354 - acc: 0.7671 - val_loss: 0.9697 - val_acc: 0.7540\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9360 - acc: 0.7655 - val_loss: 0.9688 - val_acc: 0.7540\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9348 - acc: 0.7637 - val_loss: 0.9707 - val_acc: 0.7620\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9339 - acc: 0.7660 - val_loss: 0.9634 - val_acc: 0.7540\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9339 - acc: 0.7649 - val_loss: 0.9656 - val_acc: 0.7590\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9351 - acc: 0.7643 - val_loss: 0.9669 - val_acc: 0.7580\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9324 - acc: 0.7663 - val_loss: 0.9659 - val_acc: 0.7540\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9321 - acc: 0.7648 - val_loss: 0.9712 - val_acc: 0.7530\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9315 - acc: 0.7669 - val_loss: 0.9667 - val_acc: 0.7520\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9324 - acc: 0.7651 - val_loss: 0.9720 - val_acc: 0.7530\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9327 - acc: 0.7645 - val_loss: 0.9649 - val_acc: 0.7610\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9314 - acc: 0.7660 - val_loss: 0.9601 - val_acc: 0.7570\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9314 - acc: 0.7641 - val_loss: 0.9596 - val_acc: 0.7570\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.9306 - acc: 0.7656 - val_loss: 0.9591 - val_acc: 0.7560\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9298 - acc: 0.7659 - val_loss: 0.9616 - val_acc: 0.7540\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9299 - acc: 0.7645 - val_loss: 0.9616 - val_acc: 0.7520\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9300 - acc: 0.7669 - val_loss: 0.9629 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9288 - acc: 0.7668 - val_loss: 0.9644 - val_acc: 0.7570\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9279 - acc: 0.7653 - val_loss: 0.9592 - val_acc: 0.7590\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9286 - acc: 0.7656 - val_loss: 0.9692 - val_acc: 0.7500\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9269 - acc: 0.7664 - val_loss: 0.9592 - val_acc: 0.7530\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9280 - acc: 0.7652 - val_loss: 0.9613 - val_acc: 0.7530\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9267 - acc: 0.7676 - val_loss: 0.9619 - val_acc: 0.7500\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9263 - acc: 0.7669 - val_loss: 0.9596 - val_acc: 0.7490\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9248 - acc: 0.7692 - val_loss: 0.9569 - val_acc: 0.7610\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9244 - acc: 0.7668 - val_loss: 0.9621 - val_acc: 0.7540\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9246 - acc: 0.7665 - val_loss: 0.9593 - val_acc: 0.7590\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9254 - acc: 0.7656 - val_loss: 0.9634 - val_acc: 0.7520\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9244 - acc: 0.7667 - val_loss: 0.9576 - val_acc: 0.7540\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9225 - acc: 0.7681 - val_loss: 0.9538 - val_acc: 0.7600\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9227 - acc: 0.7665 - val_loss: 0.9557 - val_acc: 0.7520\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9227 - acc: 0.7673 - val_loss: 0.9577 - val_acc: 0.7540\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9228 - acc: 0.7665 - val_loss: 0.9600 - val_acc: 0.7550\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9222 - acc: 0.7672 - val_loss: 0.9561 - val_acc: 0.7560\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9218 - acc: 0.7673 - val_loss: 0.9573 - val_acc: 0.7540\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9198 - acc: 0.7668 - val_loss: 0.9568 - val_acc: 0.7570\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9211 - acc: 0.7693 - val_loss: 0.9597 - val_acc: 0.7590\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9212 - acc: 0.7684 - val_loss: 0.9544 - val_acc: 0.7580\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9198 - acc: 0.7696 - val_loss: 0.9563 - val_acc: 0.7560\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9197 - acc: 0.7665 - val_loss: 0.9554 - val_acc: 0.7580\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9197 - acc: 0.7691 - val_loss: 0.9523 - val_acc: 0.7590\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9186 - acc: 0.7681 - val_loss: 0.9697 - val_acc: 0.7510\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9211 - acc: 0.7684 - val_loss: 0.9529 - val_acc: 0.7520\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9178 - acc: 0.7667 - val_loss: 0.9592 - val_acc: 0.7550\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9181 - acc: 0.7691 - val_loss: 0.9514 - val_acc: 0.7550\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9170 - acc: 0.7645 - val_loss: 0.9527 - val_acc: 0.7560\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9181 - acc: 0.7681 - val_loss: 0.9536 - val_acc: 0.7590\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9166 - acc: 0.7705 - val_loss: 0.9543 - val_acc: 0.7540\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9166 - acc: 0.7667 - val_loss: 0.9501 - val_acc: 0.7580\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9162 - acc: 0.7689 - val_loss: 0.9523 - val_acc: 0.7520\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9155 - acc: 0.7709 - val_loss: 0.9510 - val_acc: 0.7550\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9147 - acc: 0.7675 - val_loss: 0.9604 - val_acc: 0.7590\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.9155 - acc: 0.7696 - val_loss: 0.9563 - val_acc: 0.7510\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9143 - acc: 0.7707 - val_loss: 0.9605 - val_acc: 0.7450\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9141 - acc: 0.7677 - val_loss: 0.9548 - val_acc: 0.7600\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9138 - acc: 0.7723 - val_loss: 0.9601 - val_acc: 0.7520\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9141 - acc: 0.7709 - val_loss: 0.9554 - val_acc: 0.7560\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9144 - acc: 0.7663 - val_loss: 0.9512 - val_acc: 0.7540\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9144 - acc: 0.7695 - val_loss: 0.9600 - val_acc: 0.7600\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9126 - acc: 0.7685 - val_loss: 0.9771 - val_acc: 0.7470\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9146 - acc: 0.7696 - val_loss: 0.9534 - val_acc: 0.7550\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9102 - acc: 0.7703 - val_loss: 0.9545 - val_acc: 0.7510\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9104 - acc: 0.7693 - val_loss: 0.9506 - val_acc: 0.7530\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9111 - acc: 0.7704 - val_loss: 0.9637 - val_acc: 0.7580\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 0.9116 - acc: 0.7705 - val_loss: 0.9526 - val_acc: 0.7590\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9102 - acc: 0.7701 - val_loss: 0.9462 - val_acc: 0.7580\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9102 - acc: 0.7705 - val_loss: 0.9619 - val_acc: 0.7510\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9102 - acc: 0.7735 - val_loss: 0.9555 - val_acc: 0.7560\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9086 - acc: 0.7699 - val_loss: 0.9563 - val_acc: 0.7490\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9081 - acc: 0.7719 - val_loss: 0.9539 - val_acc: 0.7590\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9092 - acc: 0.7723 - val_loss: 0.9501 - val_acc: 0.7570\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9073 - acc: 0.7729 - val_loss: 0.9476 - val_acc: 0.7550\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9075 - acc: 0.7719 - val_loss: 0.9489 - val_acc: 0.7560\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9075 - acc: 0.7708 - val_loss: 0.9498 - val_acc: 0.7600\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9063 - acc: 0.7725 - val_loss: 0.9471 - val_acc: 0.7570\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.9075 - acc: 0.7741 - val_loss: 0.9503 - val_acc: 0.7550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9065 - acc: 0.7724 - val_loss: 0.9447 - val_acc: 0.7570\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9067 - acc: 0.7703 - val_loss: 0.9592 - val_acc: 0.7580\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.9058 - acc: 0.7708 - val_loss: 0.9568 - val_acc: 0.7580\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9053 - acc: 0.7720 - val_loss: 0.9459 - val_acc: 0.7600\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9061 - acc: 0.7716 - val_loss: 0.9528 - val_acc: 0.7600\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9043 - acc: 0.7724 - val_loss: 0.9507 - val_acc: 0.7610\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9041 - acc: 0.7724 - val_loss: 0.9512 - val_acc: 0.7590\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9047 - acc: 0.7716 - val_loss: 0.9475 - val_acc: 0.7520\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9031 - acc: 0.7728 - val_loss: 0.9486 - val_acc: 0.7510\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.9045 - acc: 0.7717 - val_loss: 0.9574 - val_acc: 0.7570\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9032 - acc: 0.7724 - val_loss: 0.9554 - val_acc: 0.7410\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9029 - acc: 0.7728 - val_loss: 0.9444 - val_acc: 0.7580\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9024 - acc: 0.7704 - val_loss: 0.9555 - val_acc: 0.7450\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9011 - acc: 0.7733 - val_loss: 0.9459 - val_acc: 0.7570\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9014 - acc: 0.7733 - val_loss: 0.9526 - val_acc: 0.7580\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9019 - acc: 0.7717 - val_loss: 0.9507 - val_acc: 0.7520\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9021 - acc: 0.7715 - val_loss: 0.9475 - val_acc: 0.7540\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9009 - acc: 0.7755 - val_loss: 0.9759 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9011 - acc: 0.7735 - val_loss: 0.9419 - val_acc: 0.7560\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8986 - acc: 0.7752 - val_loss: 0.9487 - val_acc: 0.7600\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8998 - acc: 0.7744 - val_loss: 0.9411 - val_acc: 0.7580\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8981 - acc: 0.7740 - val_loss: 0.9592 - val_acc: 0.7470\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8991 - acc: 0.7753 - val_loss: 0.9541 - val_acc: 0.7600\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8987 - acc: 0.7740 - val_loss: 0.9586 - val_acc: 0.7520\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8986 - acc: 0.7755 - val_loss: 0.9502 - val_acc: 0.7510\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8970 - acc: 0.7779 - val_loss: 0.9631 - val_acc: 0.7490\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8979 - acc: 0.7748 - val_loss: 0.9502 - val_acc: 0.7530\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8989 - acc: 0.7748 - val_loss: 0.9502 - val_acc: 0.7600\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8964 - acc: 0.7752 - val_loss: 0.9420 - val_acc: 0.7600\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8968 - acc: 0.7744 - val_loss: 0.9416 - val_acc: 0.7480\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8950 - acc: 0.7740 - val_loss: 0.9486 - val_acc: 0.7550\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8976 - acc: 0.7723 - val_loss: 0.9450 - val_acc: 0.7490\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8955 - acc: 0.7735 - val_loss: 0.9460 - val_acc: 0.7540\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8965 - acc: 0.7763 - val_loss: 0.9379 - val_acc: 0.7540\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8933 - acc: 0.7748 - val_loss: 0.9515 - val_acc: 0.7510\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8936 - acc: 0.7764 - val_loss: 0.9416 - val_acc: 0.7540\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8937 - acc: 0.7743 - val_loss: 0.9576 - val_acc: 0.7490\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8963 - acc: 0.7745 - val_loss: 0.9592 - val_acc: 0.7540\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8934 - acc: 0.7772 - val_loss: 0.9478 - val_acc: 0.7500\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8934 - acc: 0.7745 - val_loss: 0.9416 - val_acc: 0.7520\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8930 - acc: 0.7765 - val_loss: 0.9437 - val_acc: 0.7600\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8934 - acc: 0.7768 - val_loss: 0.9386 - val_acc: 0.7590\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8914 - acc: 0.7752 - val_loss: 0.9414 - val_acc: 0.7530\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8917 - acc: 0.7773 - val_loss: 0.9392 - val_acc: 0.7590\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8895 - acc: 0.7777 - val_loss: 0.9430 - val_acc: 0.7550\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8909 - acc: 0.7739 - val_loss: 0.9453 - val_acc: 0.7610\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8910 - acc: 0.7772 - val_loss: 0.9398 - val_acc: 0.7550\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8902 - acc: 0.7768 - val_loss: 0.9363 - val_acc: 0.7570\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8903 - acc: 0.7781 - val_loss: 0.9420 - val_acc: 0.7530\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8899 - acc: 0.7772 - val_loss: 0.9653 - val_acc: 0.7510\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8914 - acc: 0.7777 - val_loss: 0.9383 - val_acc: 0.7560\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8891 - acc: 0.7761 - val_loss: 0.9389 - val_acc: 0.7510\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8880 - acc: 0.7765 - val_loss: 0.9415 - val_acc: 0.7560\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8903 - acc: 0.7751 - val_loss: 0.9473 - val_acc: 0.7440\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8881 - acc: 0.7784 - val_loss: 0.9508 - val_acc: 0.7520\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8882 - acc: 0.7764 - val_loss: 0.9440 - val_acc: 0.7480\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8879 - acc: 0.7755 - val_loss: 0.9428 - val_acc: 0.7520\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8884 - acc: 0.7791 - val_loss: 0.9440 - val_acc: 0.7580\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8876 - acc: 0.7792 - val_loss: 0.9485 - val_acc: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8874 - acc: 0.7775 - val_loss: 0.9455 - val_acc: 0.7590\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8884 - acc: 0.7757 - val_loss: 0.9387 - val_acc: 0.7540\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8863 - acc: 0.7768 - val_loss: 0.9432 - val_acc: 0.7450\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8879 - acc: 0.7752 - val_loss: 0.9398 - val_acc: 0.7570\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8846 - acc: 0.7768 - val_loss: 0.9518 - val_acc: 0.7500\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8865 - acc: 0.7799 - val_loss: 0.9398 - val_acc: 0.7520\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8845 - acc: 0.7787 - val_loss: 0.9456 - val_acc: 0.7550\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8838 - acc: 0.7801 - val_loss: 0.9452 - val_acc: 0.7540\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8856 - acc: 0.7816 - val_loss: 0.9360 - val_acc: 0.7530\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8835 - acc: 0.7804 - val_loss: 0.9339 - val_acc: 0.7520\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8820 - acc: 0.7795 - val_loss: 0.9392 - val_acc: 0.7550\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8830 - acc: 0.7797 - val_loss: 0.9337 - val_acc: 0.7580\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8857 - acc: 0.7777 - val_loss: 0.9323 - val_acc: 0.7560\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8817 - acc: 0.7813 - val_loss: 0.9333 - val_acc: 0.7540\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8809 - acc: 0.7780 - val_loss: 0.9429 - val_acc: 0.7520\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8819 - acc: 0.7827 - val_loss: 0.9398 - val_acc: 0.7580\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8838 - acc: 0.7792 - val_loss: 0.9468 - val_acc: 0.7540\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8818 - acc: 0.7797 - val_loss: 0.9493 - val_acc: 0.7430\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8817 - acc: 0.7777 - val_loss: 0.9405 - val_acc: 0.7570\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8800 - acc: 0.7809 - val_loss: 0.9490 - val_acc: 0.7500\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8807 - acc: 0.7793 - val_loss: 0.9420 - val_acc: 0.7570\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8812 - acc: 0.7784 - val_loss: 0.9342 - val_acc: 0.7610\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8786 - acc: 0.7824 - val_loss: 0.9486 - val_acc: 0.7500\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8799 - acc: 0.7777 - val_loss: 0.9328 - val_acc: 0.7550\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8795 - acc: 0.7823 - val_loss: 0.9320 - val_acc: 0.7550\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8787 - acc: 0.7805 - val_loss: 0.9403 - val_acc: 0.7490\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8779 - acc: 0.7817 - val_loss: 0.9384 - val_acc: 0.7530\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8784 - acc: 0.7788 - val_loss: 0.9421 - val_acc: 0.7440\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8790 - acc: 0.7819 - val_loss: 0.9438 - val_acc: 0.7450\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8790 - acc: 0.7807 - val_loss: 0.9549 - val_acc: 0.7400\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8773 - acc: 0.7835 - val_loss: 0.9353 - val_acc: 0.7600\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8770 - acc: 0.7801 - val_loss: 0.9664 - val_acc: 0.7520\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8780 - acc: 0.7797 - val_loss: 0.9359 - val_acc: 0.7640\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8773 - acc: 0.7820 - val_loss: 0.9375 - val_acc: 0.7560\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8750 - acc: 0.7803 - val_loss: 0.9300 - val_acc: 0.7580\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8748 - acc: 0.7840 - val_loss: 0.9389 - val_acc: 0.7520\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8752 - acc: 0.7817 - val_loss: 0.9559 - val_acc: 0.7490\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8781 - acc: 0.7816 - val_loss: 0.9351 - val_acc: 0.7570\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8764 - acc: 0.7796 - val_loss: 0.9348 - val_acc: 0.7560\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8743 - acc: 0.7815 - val_loss: 0.9355 - val_acc: 0.7500\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8756 - acc: 0.7823 - val_loss: 0.9472 - val_acc: 0.7480\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8746 - acc: 0.7827 - val_loss: 0.9444 - val_acc: 0.7560\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8744 - acc: 0.7859 - val_loss: 0.9385 - val_acc: 0.7620\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8735 - acc: 0.7797 - val_loss: 0.9352 - val_acc: 0.7500\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8735 - acc: 0.7816 - val_loss: 0.9474 - val_acc: 0.7550\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8732 - acc: 0.7803 - val_loss: 0.9410 - val_acc: 0.7600\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8729 - acc: 0.7845 - val_loss: 0.9333 - val_acc: 0.7480\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8747 - acc: 0.7823 - val_loss: 0.9345 - val_acc: 0.7530\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8719 - acc: 0.7803 - val_loss: 0.9378 - val_acc: 0.7480\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8739 - acc: 0.7840 - val_loss: 0.9311 - val_acc: 0.7560\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8711 - acc: 0.7837 - val_loss: 0.9636 - val_acc: 0.7540\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8726 - acc: 0.7835 - val_loss: 0.9369 - val_acc: 0.7560\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8708 - acc: 0.7841 - val_loss: 0.9273 - val_acc: 0.7540\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8707 - acc: 0.7837 - val_loss: 0.9515 - val_acc: 0.7530\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8696 - acc: 0.7847 - val_loss: 0.9363 - val_acc: 0.7440\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8691 - acc: 0.7851 - val_loss: 0.9352 - val_acc: 0.7520\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8710 - acc: 0.7835 - val_loss: 0.9412 - val_acc: 0.7600\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8702 - acc: 0.7860 - val_loss: 0.9349 - val_acc: 0.7530\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8693 - acc: 0.7841 - val_loss: 0.9336 - val_acc: 0.7560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8717 - acc: 0.7811 - val_loss: 0.9392 - val_acc: 0.7570\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8690 - acc: 0.7832 - val_loss: 0.9444 - val_acc: 0.7550\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8681 - acc: 0.7880 - val_loss: 0.9336 - val_acc: 0.7500\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8672 - acc: 0.7851 - val_loss: 0.9342 - val_acc: 0.7510\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8681 - acc: 0.7863 - val_loss: 0.9313 - val_acc: 0.7550\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8673 - acc: 0.7860 - val_loss: 0.9298 - val_acc: 0.7550\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8665 - acc: 0.7848 - val_loss: 0.9403 - val_acc: 0.7480\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8665 - acc: 0.7853 - val_loss: 0.9455 - val_acc: 0.7550\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8650 - acc: 0.7853 - val_loss: 0.9413 - val_acc: 0.7550\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8690 - acc: 0.7859 - val_loss: 0.9408 - val_acc: 0.7610\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8670 - acc: 0.7857 - val_loss: 0.9568 - val_acc: 0.7560\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8665 - acc: 0.7873 - val_loss: 0.9271 - val_acc: 0.7540\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8653 - acc: 0.7811 - val_loss: 0.9251 - val_acc: 0.7560\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8656 - acc: 0.7861 - val_loss: 0.9565 - val_acc: 0.7340\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8664 - acc: 0.7841 - val_loss: 0.9288 - val_acc: 0.7580\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8641 - acc: 0.7873 - val_loss: 0.9381 - val_acc: 0.7460\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8654 - acc: 0.7857 - val_loss: 0.9364 - val_acc: 0.7490\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8642 - acc: 0.7881 - val_loss: 0.9357 - val_acc: 0.7550\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8634 - acc: 0.7876 - val_loss: 0.9458 - val_acc: 0.7420\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8650 - acc: 0.7865 - val_loss: 0.9379 - val_acc: 0.7590\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8655 - acc: 0.7857 - val_loss: 0.9312 - val_acc: 0.7530\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8635 - acc: 0.7861 - val_loss: 0.9312 - val_acc: 0.7570\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8631 - acc: 0.7875 - val_loss: 0.9527 - val_acc: 0.7490\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8646 - acc: 0.7860 - val_loss: 0.9536 - val_acc: 0.7540\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8666 - acc: 0.7861 - val_loss: 0.9338 - val_acc: 0.7540\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8616 - acc: 0.7884 - val_loss: 0.9366 - val_acc: 0.7440\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8619 - acc: 0.7908 - val_loss: 0.9470 - val_acc: 0.7480\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8615 - acc: 0.7928 - val_loss: 0.9330 - val_acc: 0.7560\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8619 - acc: 0.7895 - val_loss: 0.9286 - val_acc: 0.7510\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8607 - acc: 0.7883 - val_loss: 0.9363 - val_acc: 0.7520\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8624 - acc: 0.7877 - val_loss: 0.9324 - val_acc: 0.7580\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8612 - acc: 0.7851 - val_loss: 0.9384 - val_acc: 0.7460\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8604 - acc: 0.7896 - val_loss: 0.9315 - val_acc: 0.7530\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8595 - acc: 0.7893 - val_loss: 0.9243 - val_acc: 0.7560\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8590 - acc: 0.7880 - val_loss: 0.9249 - val_acc: 0.7560\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 0.8604 - acc: 0.7887 - val_loss: 0.9445 - val_acc: 0.7460\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8584 - acc: 0.7908 - val_loss: 0.9359 - val_acc: 0.7560\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8578 - acc: 0.7912 - val_loss: 0.9337 - val_acc: 0.7560\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8617 - acc: 0.7909 - val_loss: 0.9258 - val_acc: 0.7620\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8578 - acc: 0.7911 - val_loss: 0.9411 - val_acc: 0.7490\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8590 - acc: 0.7889 - val_loss: 0.9283 - val_acc: 0.7530\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8592 - acc: 0.7891 - val_loss: 0.9355 - val_acc: 0.7570\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8567 - acc: 0.7883 - val_loss: 0.9431 - val_acc: 0.7520\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8566 - acc: 0.7893 - val_loss: 0.9472 - val_acc: 0.7450\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8579 - acc: 0.7861 - val_loss: 0.9364 - val_acc: 0.7590\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8571 - acc: 0.7879 - val_loss: 0.9457 - val_acc: 0.7500\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8563 - acc: 0.7893 - val_loss: 0.9237 - val_acc: 0.7510\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8546 - acc: 0.7887 - val_loss: 0.9291 - val_acc: 0.7480\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.8555 - acc: 0.7896 - val_loss: 0.9516 - val_acc: 0.7490\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.8576 - acc: 0.7895 - val_loss: 0.9272 - val_acc: 0.7560\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8562 - acc: 0.7923 - val_loss: 0.9340 - val_acc: 0.7530\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 0.8542 - acc: 0.7892 - val_loss: 0.9714 - val_acc: 0.7450\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8575 - acc: 0.7871 - val_loss: 0.9202 - val_acc: 0.7610\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8557 - acc: 0.7888 - val_loss: 0.9255 - val_acc: 0.7470\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8557 - acc: 0.7925 - val_loss: 0.9255 - val_acc: 0.7580\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.8547 - acc: 0.7915 - val_loss: 0.9247 - val_acc: 0.7590\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8537 - acc: 0.7940 - val_loss: 0.9258 - val_acc: 0.7580\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8511 - acc: 0.7908 - val_loss: 0.9259 - val_acc: 0.7480\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8540 - acc: 0.7900 - val_loss: 0.9440 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8530 - acc: 0.7889 - val_loss: 0.9395 - val_acc: 0.7570\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8575 - acc: 0.7891 - val_loss: 0.9286 - val_acc: 0.7460\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8550 - acc: 0.7884 - val_loss: 0.9348 - val_acc: 0.7520\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8524 - acc: 0.7904 - val_loss: 0.9481 - val_acc: 0.7450\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8524 - acc: 0.7889 - val_loss: 0.9276 - val_acc: 0.7520\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8512 - acc: 0.7904 - val_loss: 0.9231 - val_acc: 0.7600\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8502 - acc: 0.7923 - val_loss: 0.9314 - val_acc: 0.7440\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8516 - acc: 0.7916 - val_loss: 0.9590 - val_acc: 0.7570\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8504 - acc: 0.7953 - val_loss: 0.9348 - val_acc: 0.7490\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8496 - acc: 0.7887 - val_loss: 0.9240 - val_acc: 0.7500\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8530 - acc: 0.7905 - val_loss: 0.9255 - val_acc: 0.7500\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8502 - acc: 0.7912 - val_loss: 0.9204 - val_acc: 0.7570\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8483 - acc: 0.7927 - val_loss: 0.9257 - val_acc: 0.7630\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8482 - acc: 0.7937 - val_loss: 0.9422 - val_acc: 0.7460\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8551 - acc: 0.7883 - val_loss: 0.9352 - val_acc: 0.7530\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8509 - acc: 0.7919 - val_loss: 0.9219 - val_acc: 0.7530\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8491 - acc: 0.7945 - val_loss: 0.9401 - val_acc: 0.7560\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8474 - acc: 0.7915 - val_loss: 0.9226 - val_acc: 0.7640\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8488 - acc: 0.7928 - val_loss: 0.9431 - val_acc: 0.7420\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8483 - acc: 0.7936 - val_loss: 0.9239 - val_acc: 0.7540\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8467 - acc: 0.7920 - val_loss: 0.9182 - val_acc: 0.7560\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8474 - acc: 0.7936 - val_loss: 0.9246 - val_acc: 0.7530\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 0.8468 - acc: 0.7927 - val_loss: 0.9195 - val_acc: 0.7550\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8482 - acc: 0.7923 - val_loss: 0.9323 - val_acc: 0.7450\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8454 - acc: 0.7969 - val_loss: 0.9199 - val_acc: 0.7550\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.8461 - acc: 0.7963 - val_loss: 0.9218 - val_acc: 0.7650\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8460 - acc: 0.7969 - val_loss: 0.9281 - val_acc: 0.7520\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8491 - acc: 0.7883 - val_loss: 0.9229 - val_acc: 0.7590\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8460 - acc: 0.7936 - val_loss: 0.9428 - val_acc: 0.7430\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8459 - acc: 0.7957 - val_loss: 0.9702 - val_acc: 0.7560\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8460 - acc: 0.7947 - val_loss: 0.9301 - val_acc: 0.7500\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8450 - acc: 0.7965 - val_loss: 0.9240 - val_acc: 0.7540\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8441 - acc: 0.7920 - val_loss: 0.9188 - val_acc: 0.7510\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8446 - acc: 0.7960 - val_loss: 0.9185 - val_acc: 0.7590\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8464 - acc: 0.7959 - val_loss: 0.9294 - val_acc: 0.7670\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8426 - acc: 0.7953 - val_loss: 0.9274 - val_acc: 0.7590\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8439 - acc: 0.7949 - val_loss: 0.9283 - val_acc: 0.7450\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8423 - acc: 0.7943 - val_loss: 0.9320 - val_acc: 0.7450\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8418 - acc: 0.7923 - val_loss: 0.9328 - val_acc: 0.7590\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8408 - acc: 0.7961 - val_loss: 0.9146 - val_acc: 0.7610\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8434 - acc: 0.7961 - val_loss: 0.9412 - val_acc: 0.7330\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8438 - acc: 0.7965 - val_loss: 0.9437 - val_acc: 0.7540\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8440 - acc: 0.7965 - val_loss: 0.9311 - val_acc: 0.7510\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8434 - acc: 0.7984 - val_loss: 0.9184 - val_acc: 0.7570\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8427 - acc: 0.7923 - val_loss: 0.9233 - val_acc: 0.7580\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8399 - acc: 0.7948 - val_loss: 0.9211 - val_acc: 0.7610\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8452 - acc: 0.7932 - val_loss: 0.9376 - val_acc: 0.7550\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8442 - acc: 0.7924 - val_loss: 0.9205 - val_acc: 0.7590\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8404 - acc: 0.7935 - val_loss: 0.9339 - val_acc: 0.7580\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8407 - acc: 0.7945 - val_loss: 0.9588 - val_acc: 0.7460\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8407 - acc: 0.7960 - val_loss: 0.9166 - val_acc: 0.7680\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8396 - acc: 0.7953 - val_loss: 0.9409 - val_acc: 0.7630\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8412 - acc: 0.7977 - val_loss: 0.9248 - val_acc: 0.7610\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8386 - acc: 0.7993 - val_loss: 0.9188 - val_acc: 0.7510\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8415 - acc: 0.7972 - val_loss: 0.9874 - val_acc: 0.7410\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8430 - acc: 0.7959 - val_loss: 0.9303 - val_acc: 0.7490\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8384 - acc: 0.7960 - val_loss: 0.9174 - val_acc: 0.7520\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8373 - acc: 0.7975 - val_loss: 0.9198 - val_acc: 0.7600\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8394 - acc: 0.7985 - val_loss: 0.9213 - val_acc: 0.7460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8372 - acc: 0.7965 - val_loss: 0.9520 - val_acc: 0.7620\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8388 - acc: 0.7961 - val_loss: 0.9312 - val_acc: 0.7490\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8407 - acc: 0.7956 - val_loss: 0.9762 - val_acc: 0.7380\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8387 - acc: 0.7963 - val_loss: 0.9169 - val_acc: 0.7550\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8397 - acc: 0.7957 - val_loss: 0.9140 - val_acc: 0.7610\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8368 - acc: 0.7964 - val_loss: 0.9153 - val_acc: 0.7570\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8370 - acc: 0.7960 - val_loss: 0.9182 - val_acc: 0.7590\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8369 - acc: 0.7955 - val_loss: 0.9303 - val_acc: 0.7530\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8382 - acc: 0.7969 - val_loss: 0.9123 - val_acc: 0.7610\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8337 - acc: 0.8003 - val_loss: 0.9228 - val_acc: 0.7480\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8362 - acc: 0.7961 - val_loss: 0.9169 - val_acc: 0.7460\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8352 - acc: 0.7979 - val_loss: 0.9197 - val_acc: 0.7580\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8351 - acc: 0.7980 - val_loss: 0.9308 - val_acc: 0.7480\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8367 - acc: 0.7971 - val_loss: 0.9462 - val_acc: 0.7680\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8343 - acc: 0.7991 - val_loss: 0.9134 - val_acc: 0.7610\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8345 - acc: 0.7993 - val_loss: 0.9373 - val_acc: 0.7530\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8352 - acc: 0.7997 - val_loss: 0.9387 - val_acc: 0.7540\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8321 - acc: 0.7968 - val_loss: 0.9318 - val_acc: 0.7560\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8350 - acc: 0.7965 - val_loss: 0.9249 - val_acc: 0.7520\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8338 - acc: 0.7980 - val_loss: 0.9272 - val_acc: 0.7400\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8376 - acc: 0.7977 - val_loss: 0.9175 - val_acc: 0.7710\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8342 - acc: 0.7973 - val_loss: 0.9418 - val_acc: 0.7320\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8361 - acc: 0.7993 - val_loss: 0.9735 - val_acc: 0.7270\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8334 - acc: 0.7991 - val_loss: 0.9389 - val_acc: 0.7500\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8357 - acc: 0.7968 - val_loss: 0.9653 - val_acc: 0.7540\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8351 - acc: 0.7967 - val_loss: 0.9186 - val_acc: 0.7600\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8305 - acc: 0.7991 - val_loss: 0.9200 - val_acc: 0.7480\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8299 - acc: 0.7983 - val_loss: 0.9309 - val_acc: 0.7550\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 0.8310 - acc: 0.7997 - val_loss: 0.9324 - val_acc: 0.7540\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8318 - acc: 0.7968 - val_loss: 0.9456 - val_acc: 0.7620\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8384 - acc: 0.7961 - val_loss: 0.9388 - val_acc: 0.7550\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8342 - acc: 0.7996 - val_loss: 0.9339 - val_acc: 0.7410\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8315 - acc: 0.7956 - val_loss: 0.9331 - val_acc: 0.7400\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8300 - acc: 0.8019 - val_loss: 0.9321 - val_acc: 0.7650\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8278 - acc: 0.8013 - val_loss: 0.9120 - val_acc: 0.7630\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8285 - acc: 0.8001 - val_loss: 0.9181 - val_acc: 0.7570\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8323 - acc: 0.7965 - val_loss: 0.9544 - val_acc: 0.7540\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8292 - acc: 0.8001 - val_loss: 0.9465 - val_acc: 0.7390\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8332 - acc: 0.7988 - val_loss: 0.9359 - val_acc: 0.7670\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8282 - acc: 0.8021 - val_loss: 0.9139 - val_acc: 0.7600\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8304 - acc: 0.7985 - val_loss: 0.9255 - val_acc: 0.7650\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8292 - acc: 0.7999 - val_loss: 0.9145 - val_acc: 0.7630\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8285 - acc: 0.8007 - val_loss: 0.9167 - val_acc: 0.7760\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8268 - acc: 0.7975 - val_loss: 0.9745 - val_acc: 0.7420\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8306 - acc: 0.7992 - val_loss: 0.9491 - val_acc: 0.7360\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8281 - acc: 0.7987 - val_loss: 0.9172 - val_acc: 0.7540\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8278 - acc: 0.8008 - val_loss: 0.9201 - val_acc: 0.7480\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8266 - acc: 0.7996 - val_loss: 0.9405 - val_acc: 0.7540\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 0.8272 - acc: 0.7976 - val_loss: 0.9209 - val_acc: 0.7580\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8247 - acc: 0.7984 - val_loss: 0.9085 - val_acc: 0.7690\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8229 - acc: 0.8012 - val_loss: 0.9108 - val_acc: 0.7600\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8266 - acc: 0.7984 - val_loss: 0.9213 - val_acc: 0.7430\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8252 - acc: 0.7999 - val_loss: 0.9252 - val_acc: 0.7670\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8244 - acc: 0.8009 - val_loss: 0.9131 - val_acc: 0.7560\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8256 - acc: 0.8009 - val_loss: 0.9128 - val_acc: 0.7740\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8256 - acc: 0.8029 - val_loss: 0.9158 - val_acc: 0.7660\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8246 - acc: 0.8045 - val_loss: 0.9252 - val_acc: 0.7530\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8247 - acc: 0.7999 - val_loss: 0.9117 - val_acc: 0.7640\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8255 - acc: 0.8008 - val_loss: 0.9221 - val_acc: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8235 - acc: 0.8004 - val_loss: 0.9061 - val_acc: 0.7600\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8240 - acc: 0.8015 - val_loss: 0.9060 - val_acc: 0.7650\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 0.8236 - acc: 0.7996 - val_loss: 0.9250 - val_acc: 0.7540\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8260 - acc: 0.8009 - val_loss: 0.9910 - val_acc: 0.7360\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8323 - acc: 0.7993 - val_loss: 0.9117 - val_acc: 0.7570\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8222 - acc: 0.8036 - val_loss: 0.9262 - val_acc: 0.7410\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8244 - acc: 0.8015 - val_loss: 0.9303 - val_acc: 0.7510\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8259 - acc: 0.8017 - val_loss: 0.9169 - val_acc: 0.7520\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8210 - acc: 0.8044 - val_loss: 0.9209 - val_acc: 0.7500\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8229 - acc: 0.8023 - val_loss: 0.9185 - val_acc: 0.7590\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8225 - acc: 0.8044 - val_loss: 0.9160 - val_acc: 0.7630\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8210 - acc: 0.8033 - val_loss: 0.9342 - val_acc: 0.7480\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8216 - acc: 0.8032 - val_loss: 0.9134 - val_acc: 0.7470\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8216 - acc: 0.8045 - val_loss: 0.9165 - val_acc: 0.7670\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8233 - acc: 0.8035 - val_loss: 0.9393 - val_acc: 0.7400\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8226 - acc: 0.8025 - val_loss: 0.9160 - val_acc: 0.7610\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8231 - acc: 0.8032 - val_loss: 0.9436 - val_acc: 0.7390\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8193 - acc: 0.8027 - val_loss: 0.9669 - val_acc: 0.7500\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8243 - acc: 0.8013 - val_loss: 0.9106 - val_acc: 0.7560\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8212 - acc: 0.8055 - val_loss: 0.9123 - val_acc: 0.7560\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8212 - acc: 0.8023 - val_loss: 0.9260 - val_acc: 0.7560\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8192 - acc: 0.8057 - val_loss: 0.9082 - val_acc: 0.7650\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8211 - acc: 0.7995 - val_loss: 0.9431 - val_acc: 0.7390\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8172 - acc: 0.8071 - val_loss: 0.9085 - val_acc: 0.7500\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8208 - acc: 0.8017 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8234 - acc: 0.8047 - val_loss: 0.9083 - val_acc: 0.7570\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8180 - acc: 0.8040 - val_loss: 0.9565 - val_acc: 0.7510\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8199 - acc: 0.8061 - val_loss: 0.9140 - val_acc: 0.7590\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.8180 - acc: 0.8068 - val_loss: 0.9174 - val_acc: 0.7530\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8181 - acc: 0.8047 - val_loss: 0.9347 - val_acc: 0.7510\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8217 - acc: 0.8045 - val_loss: 0.9200 - val_acc: 0.7520\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8156 - acc: 0.8076 - val_loss: 0.9106 - val_acc: 0.7650\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8168 - acc: 0.8029 - val_loss: 0.9253 - val_acc: 0.7610\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.8178 - acc: 0.8047 - val_loss: 0.9698 - val_acc: 0.7480\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8254 - acc: 0.8035 - val_loss: 0.9231 - val_acc: 0.7520\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8226 - acc: 0.8001 - val_loss: 0.9345 - val_acc: 0.7690\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8161 - acc: 0.8055 - val_loss: 0.9197 - val_acc: 0.7590\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8162 - acc: 0.8043 - val_loss: 0.9229 - val_acc: 0.7470\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8204 - acc: 0.8039 - val_loss: 0.9411 - val_acc: 0.7360\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8187 - acc: 0.8053 - val_loss: 0.9271 - val_acc: 0.7460\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8161 - acc: 0.8035 - val_loss: 0.9053 - val_acc: 0.7550\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8145 - acc: 0.8069 - val_loss: 0.9111 - val_acc: 0.7640\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8170 - acc: 0.8015 - val_loss: 0.9064 - val_acc: 0.7570\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8144 - acc: 0.8059 - val_loss: 0.9275 - val_acc: 0.7520\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8197 - acc: 0.8035 - val_loss: 0.9203 - val_acc: 0.7430\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8116 - acc: 0.8051 - val_loss: 0.9125 - val_acc: 0.7550\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8125 - acc: 0.8049 - val_loss: 0.9278 - val_acc: 0.7690\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8197 - acc: 0.8035 - val_loss: 0.9214 - val_acc: 0.7660\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8122 - acc: 0.8068 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8140 - acc: 0.8085 - val_loss: 0.9296 - val_acc: 0.7390\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8144 - acc: 0.8036 - val_loss: 0.9578 - val_acc: 0.7430\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8166 - acc: 0.8028 - val_loss: 0.9787 - val_acc: 0.7400\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8125 - acc: 0.8072 - val_loss: 0.9340 - val_acc: 0.7440\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8155 - acc: 0.8051 - val_loss: 0.9080 - val_acc: 0.7600\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8103 - acc: 0.8081 - val_loss: 0.9055 - val_acc: 0.7650\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8148 - acc: 0.8061 - val_loss: 0.9160 - val_acc: 0.7440\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8123 - acc: 0.8099 - val_loss: 0.9050 - val_acc: 0.7600\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8146 - acc: 0.8045 - val_loss: 0.9437 - val_acc: 0.7490\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8164 - acc: 0.8036 - val_loss: 0.9123 - val_acc: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8120 - acc: 0.8029 - val_loss: 0.9091 - val_acc: 0.7730\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8124 - acc: 0.8056 - val_loss: 0.9193 - val_acc: 0.7510\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8090 - acc: 0.8091 - val_loss: 0.9509 - val_acc: 0.7410\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8137 - acc: 0.8057 - val_loss: 0.9137 - val_acc: 0.7620\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8090 - acc: 0.8088 - val_loss: 0.9019 - val_acc: 0.7630\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8082 - acc: 0.8107 - val_loss: 0.9037 - val_acc: 0.7600\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8125 - acc: 0.8015 - val_loss: 0.9573 - val_acc: 0.7420\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8143 - acc: 0.8052 - val_loss: 0.9209 - val_acc: 0.7570\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 0.8171 - acc: 0.8052 - val_loss: 0.9188 - val_acc: 0.7630\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8160 - acc: 0.8036 - val_loss: 0.9279 - val_acc: 0.7770\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8177 - acc: 0.8083 - val_loss: 0.9742 - val_acc: 0.7360\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8193 - acc: 0.8041 - val_loss: 0.9052 - val_acc: 0.7620\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8097 - acc: 0.8101 - val_loss: 0.9069 - val_acc: 0.7550\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8094 - acc: 0.8085 - val_loss: 0.9164 - val_acc: 0.7700\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8091 - acc: 0.8052 - val_loss: 0.9227 - val_acc: 0.7530\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8129 - acc: 0.8047 - val_loss: 0.9284 - val_acc: 0.7690\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8229 - acc: 0.8024 - val_loss: 0.9195 - val_acc: 0.7570\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8083 - acc: 0.8111 - val_loss: 0.9889 - val_acc: 0.7300\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8106 - acc: 0.8095 - val_loss: 0.9318 - val_acc: 0.7590\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8111 - acc: 0.8072 - val_loss: 0.9264 - val_acc: 0.7610\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8137 - acc: 0.8051 - val_loss: 0.9130 - val_acc: 0.7510\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8061 - acc: 0.8121 - val_loss: 0.9207 - val_acc: 0.7640\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8107 - acc: 0.8105 - val_loss: 0.9302 - val_acc: 0.7670\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8076 - acc: 0.8109 - val_loss: 0.9040 - val_acc: 0.7510\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8064 - acc: 0.8095 - val_loss: 0.9077 - val_acc: 0.7590\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8072 - acc: 0.8069 - val_loss: 0.9384 - val_acc: 0.7610\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8069 - acc: 0.8097 - val_loss: 0.9124 - val_acc: 0.7590\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8091 - acc: 0.8064 - val_loss: 0.9024 - val_acc: 0.7570\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8038 - acc: 0.8117 - val_loss: 0.9078 - val_acc: 0.7770\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8091 - acc: 0.8100 - val_loss: 0.9010 - val_acc: 0.7670\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8025 - acc: 0.8104 - val_loss: 0.8982 - val_acc: 0.7560\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8044 - acc: 0.8115 - val_loss: 0.9223 - val_acc: 0.7700\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8136 - acc: 0.8037 - val_loss: 1.0728 - val_acc: 0.7080\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8196 - acc: 0.8019 - val_loss: 0.9034 - val_acc: 0.7550\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8108 - acc: 0.8079 - val_loss: 0.9154 - val_acc: 0.7600\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8071 - acc: 0.8093 - val_loss: 0.9209 - val_acc: 0.7560\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8099 - acc: 0.8083 - val_loss: 0.9052 - val_acc: 0.7630\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.8089 - acc: 0.8109 - val_loss: 0.9169 - val_acc: 0.7530\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.8020 - acc: 0.8117 - val_loss: 0.9213 - val_acc: 0.7510\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8054 - acc: 0.8141 - val_loss: 0.9094 - val_acc: 0.7530\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8050 - acc: 0.8100 - val_loss: 0.9121 - val_acc: 0.7650\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8065 - acc: 0.8077 - val_loss: 0.9620 - val_acc: 0.7490\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8232 - acc: 0.8007 - val_loss: 0.9146 - val_acc: 0.7620\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8047 - acc: 0.8103 - val_loss: 0.9131 - val_acc: 0.7520\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8046 - acc: 0.8104 - val_loss: 0.9837 - val_acc: 0.7530\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8120 - acc: 0.8067 - val_loss: 0.9232 - val_acc: 0.7660\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8023 - acc: 0.8137 - val_loss: 1.0328 - val_acc: 0.7350\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8070 - acc: 0.8097 - val_loss: 0.9112 - val_acc: 0.7650\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8030 - acc: 0.8113 - val_loss: 0.9228 - val_acc: 0.7670\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8047 - acc: 0.8119 - val_loss: 0.9228 - val_acc: 0.7670\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8098 - acc: 0.8047 - val_loss: 0.9120 - val_acc: 0.7690\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8130 - acc: 0.8040 - val_loss: 0.9474 - val_acc: 0.7500\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8086 - acc: 0.8104 - val_loss: 0.9042 - val_acc: 0.7670\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8021 - acc: 0.8120 - val_loss: 0.9167 - val_acc: 0.7560\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8039 - acc: 0.8104 - val_loss: 0.9126 - val_acc: 0.7450\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8112 - acc: 0.8063 - val_loss: 0.9912 - val_acc: 0.7310\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8174 - acc: 0.7999 - val_loss: 1.0036 - val_acc: 0.7250\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8073 - acc: 0.8127 - val_loss: 0.9360 - val_acc: 0.7490\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8050 - acc: 0.8077 - val_loss: 0.9106 - val_acc: 0.7690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8023 - acc: 0.8099 - val_loss: 0.8988 - val_acc: 0.7710\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8001 - acc: 0.8159 - val_loss: 0.9144 - val_acc: 0.7490\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8095 - acc: 0.8088 - val_loss: 1.0414 - val_acc: 0.7060\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8120 - acc: 0.8115 - val_loss: 0.9473 - val_acc: 0.7510\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8111 - acc: 0.8056 - val_loss: 0.9222 - val_acc: 0.7700\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8155 - acc: 0.8057 - val_loss: 0.9224 - val_acc: 0.7530\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8016 - acc: 0.8104 - val_loss: 0.9151 - val_acc: 0.7680\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8034 - acc: 0.8068 - val_loss: 0.9157 - val_acc: 0.7480\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7986 - acc: 0.8133 - val_loss: 0.9212 - val_acc: 0.7530\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7991 - acc: 0.8139 - val_loss: 0.9535 - val_acc: 0.7580\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8027 - acc: 0.8131 - val_loss: 0.9048 - val_acc: 0.7760\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8081 - acc: 0.8072 - val_loss: 0.9073 - val_acc: 0.7560\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8029 - acc: 0.8123 - val_loss: 0.9242 - val_acc: 0.7540\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8015 - acc: 0.8081 - val_loss: 0.9579 - val_acc: 0.7430\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8109 - acc: 0.8072 - val_loss: 0.9080 - val_acc: 0.7640\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8033 - acc: 0.8095 - val_loss: 1.0135 - val_acc: 0.7150\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8086 - acc: 0.8080 - val_loss: 0.9689 - val_acc: 0.7350\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8039 - acc: 0.8107 - val_loss: 0.8984 - val_acc: 0.7670\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8027 - acc: 0.8091 - val_loss: 1.0220 - val_acc: 0.7130\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8073 - acc: 0.8081 - val_loss: 0.9059 - val_acc: 0.7770\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8167 - acc: 0.8019 - val_loss: 0.9332 - val_acc: 0.7590\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8033 - acc: 0.8111 - val_loss: 0.9218 - val_acc: 0.7570\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8008 - acc: 0.8133 - val_loss: 0.9105 - val_acc: 0.7720\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8102 - acc: 0.8087 - val_loss: 0.9048 - val_acc: 0.7620\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7966 - acc: 0.8152 - val_loss: 1.0225 - val_acc: 0.7200\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8060 - acc: 0.8091 - val_loss: 1.0259 - val_acc: 0.7150\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8120 - acc: 0.8093 - val_loss: 0.9209 - val_acc: 0.7550\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7984 - acc: 0.8159 - val_loss: 0.9467 - val_acc: 0.7550\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8012 - acc: 0.8128 - val_loss: 0.9156 - val_acc: 0.7620\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7979 - acc: 0.8140 - val_loss: 0.9968 - val_acc: 0.7350\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8017 - acc: 0.8141 - val_loss: 0.9317 - val_acc: 0.7620\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8076 - acc: 0.8105 - val_loss: 0.9126 - val_acc: 0.7580\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8022 - acc: 0.8093 - val_loss: 1.0004 - val_acc: 0.7200\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8024 - acc: 0.8127 - val_loss: 0.9325 - val_acc: 0.7590\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8000 - acc: 0.8143 - val_loss: 0.8991 - val_acc: 0.7580\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7965 - acc: 0.8111 - val_loss: 0.9384 - val_acc: 0.7580\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8010 - acc: 0.8108 - val_loss: 0.9781 - val_acc: 0.7320\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7969 - acc: 0.8157 - val_loss: 0.9324 - val_acc: 0.7590\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8108 - acc: 0.8059 - val_loss: 0.9179 - val_acc: 0.7480\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8017 - acc: 0.8104 - val_loss: 0.9172 - val_acc: 0.7540\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7973 - acc: 0.8128 - val_loss: 0.9216 - val_acc: 0.7460\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8233 - acc: 0.8001 - val_loss: 0.9235 - val_acc: 0.7680\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.7977 - acc: 0.8143 - val_loss: 0.9187 - val_acc: 0.7520\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8039 - acc: 0.8151 - val_loss: 0.9169 - val_acc: 0.7680\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8005 - acc: 0.8088 - val_loss: 0.9123 - val_acc: 0.7480\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7955 - acc: 0.8135 - val_loss: 0.9321 - val_acc: 0.7570\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7994 - acc: 0.8121 - val_loss: 0.9211 - val_acc: 0.7490\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7977 - acc: 0.8129 - val_loss: 0.9089 - val_acc: 0.7640\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7909 - acc: 0.8179 - val_loss: 0.9519 - val_acc: 0.7510\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8017 - acc: 0.8128 - val_loss: 0.9112 - val_acc: 0.7730\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7928 - acc: 0.8151 - val_loss: 0.8982 - val_acc: 0.7650\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8023 - acc: 0.8133 - val_loss: 0.9181 - val_acc: 0.7680\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7987 - acc: 0.8129 - val_loss: 0.9333 - val_acc: 0.7590\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7998 - acc: 0.8124 - val_loss: 0.9086 - val_acc: 0.7680\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8077 - acc: 0.8072 - val_loss: 0.9285 - val_acc: 0.7530\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7986 - acc: 0.8149 - val_loss: 0.9001 - val_acc: 0.7650\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8006 - acc: 0.8140 - val_loss: 1.0032 - val_acc: 0.7210\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8066 - acc: 0.8087 - val_loss: 0.9252 - val_acc: 0.7630\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7948 - acc: 0.8092 - val_loss: 0.9574 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7934 - acc: 0.8147 - val_loss: 0.9454 - val_acc: 0.7610\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7914 - acc: 0.8192 - val_loss: 0.9504 - val_acc: 0.7630\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8052 - acc: 0.8124 - val_loss: 0.9225 - val_acc: 0.7480\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7972 - acc: 0.8151 - val_loss: 1.0583 - val_acc: 0.7140\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8120 - acc: 0.8065 - val_loss: 1.1725 - val_acc: 0.6620\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8102 - acc: 0.8095 - val_loss: 0.9136 - val_acc: 0.7610\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8021 - acc: 0.8081 - val_loss: 0.9349 - val_acc: 0.7510\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8007 - acc: 0.8068 - val_loss: 0.9263 - val_acc: 0.7630\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7973 - acc: 0.8124 - val_loss: 0.9145 - val_acc: 0.7660\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.7897 - acc: 0.8169 - val_loss: 0.9154 - val_acc: 0.7650\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7987 - acc: 0.8117 - val_loss: 0.9629 - val_acc: 0.7400\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8198 - acc: 0.8043 - val_loss: 0.9433 - val_acc: 0.7510\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8014 - acc: 0.8117 - val_loss: 0.9991 - val_acc: 0.7340\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8007 - acc: 0.8105 - val_loss: 0.9536 - val_acc: 0.7530\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7956 - acc: 0.8124 - val_loss: 0.9333 - val_acc: 0.7620\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7985 - acc: 0.8116 - val_loss: 0.9011 - val_acc: 0.7680\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7934 - acc: 0.8153 - val_loss: 0.9096 - val_acc: 0.7710\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7972 - acc: 0.8123 - val_loss: 0.9249 - val_acc: 0.7720\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7903 - acc: 0.8147 - val_loss: 0.9077 - val_acc: 0.7700\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7879 - acc: 0.8209 - val_loss: 0.9048 - val_acc: 0.7690\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8039 - acc: 0.8109 - val_loss: 0.9049 - val_acc: 0.7620\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7927 - acc: 0.8167 - val_loss: 0.9308 - val_acc: 0.7570\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7969 - acc: 0.8120 - val_loss: 0.8988 - val_acc: 0.7610\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7939 - acc: 0.8193 - val_loss: 0.9343 - val_acc: 0.7470\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.7891 - acc: 0.8197 - val_loss: 0.9299 - val_acc: 0.7580\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.7919 - acc: 0.8160 - val_loss: 0.9029 - val_acc: 0.7610\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8038 - acc: 0.8132 - val_loss: 0.8994 - val_acc: 0.7730\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7889 - acc: 0.8179 - val_loss: 0.9669 - val_acc: 0.7620\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7931 - acc: 0.8161 - val_loss: 0.9162 - val_acc: 0.7680\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8000 - acc: 0.8087 - val_loss: 0.8966 - val_acc: 0.7690\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8044 - acc: 0.8088 - val_loss: 0.9050 - val_acc: 0.7610\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7954 - acc: 0.8161 - val_loss: 0.8948 - val_acc: 0.7650\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8021 - acc: 0.8099 - val_loss: 0.9645 - val_acc: 0.7320\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7949 - acc: 0.8132 - val_loss: 0.8993 - val_acc: 0.7660\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7913 - acc: 0.8112 - val_loss: 0.9181 - val_acc: 0.7530\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7941 - acc: 0.8140 - val_loss: 0.9430 - val_acc: 0.7530\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7863 - acc: 0.8185 - val_loss: 0.8989 - val_acc: 0.7620\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7921 - acc: 0.8155 - val_loss: 0.9671 - val_acc: 0.7440\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8209 - acc: 0.8009 - val_loss: 0.9433 - val_acc: 0.7370\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8023 - acc: 0.8121 - val_loss: 0.9292 - val_acc: 0.7560\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7911 - acc: 0.8171 - val_loss: 0.9875 - val_acc: 0.7490\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 0.7934 - acc: 0.8167 - val_loss: 0.9714 - val_acc: 0.7330\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 0.8001 - acc: 0.8129 - val_loss: 0.9091 - val_acc: 0.7770\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7960 - acc: 0.8163 - val_loss: 1.0402 - val_acc: 0.7070\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7996 - acc: 0.8109 - val_loss: 0.9590 - val_acc: 0.7420\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8072 - acc: 0.8091 - val_loss: 0.9132 - val_acc: 0.7680\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.7925 - acc: 0.8148 - val_loss: 0.9938 - val_acc: 0.7460\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8192 - acc: 0.7981 - val_loss: 0.9136 - val_acc: 0.7660\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.7837 - acc: 0.8180 - val_loss: 0.8981 - val_acc: 0.7690\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7876 - acc: 0.8193 - val_loss: 0.9288 - val_acc: 0.7550\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.7862 - acc: 0.8185 - val_loss: 0.9158 - val_acc: 0.7630\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.7890 - acc: 0.8173 - val_loss: 0.9476 - val_acc: 0.7540\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7878 - acc: 0.8153 - val_loss: 0.9172 - val_acc: 0.7630\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7983 - acc: 0.8132 - val_loss: 0.9321 - val_acc: 0.7620\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7881 - acc: 0.8177 - val_loss: 0.9259 - val_acc: 0.7620\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8048 - acc: 0.8080 - val_loss: 0.9245 - val_acc: 0.7650\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, kernel_regularizer=regularizers.l1(0.005), activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training Acc. L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation Acc. L1')\n",
    "plt.title('Training & Validation Accuracy L2 vs. Regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 97us/step\n",
      "1500/1500 [==============================] - 0s 106us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8097889014561971, 0.8117333333015442]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9774045054117838, 0.7373333336512248]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 4s 515us/step - loss: 1.9884 - acc: 0.1308 - val_loss: 1.9407 - val_acc: 0.1660\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 1.9678 - acc: 0.1480 - val_loss: 1.9300 - val_acc: 0.1830\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 2s 240us/step - loss: 1.9551 - acc: 0.1524 - val_loss: 1.9231 - val_acc: 0.1910\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 2s 237us/step - loss: 1.9448 - acc: 0.1649 - val_loss: 1.9170 - val_acc: 0.2110\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 2s 266us/step - loss: 1.9372 - acc: 0.1765 - val_loss: 1.9113 - val_acc: 0.2190\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 2s 237us/step - loss: 1.9268 - acc: 0.1907 - val_loss: 1.9047 - val_acc: 0.2340\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 1.9272 - acc: 0.1899 - val_loss: 1.8982 - val_acc: 0.2400\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 1.9144 - acc: 0.2013 - val_loss: 1.8910 - val_acc: 0.2460\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.9086 - acc: 0.2100 - val_loss: 1.8816 - val_acc: 0.2480\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 1.9030 - acc: 0.2108 - val_loss: 1.8723 - val_acc: 0.2610\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 2s 273us/step - loss: 1.8934 - acc: 0.2224 - val_loss: 1.8621 - val_acc: 0.2710\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 1.8882 - acc: 0.2236 - val_loss: 1.8515 - val_acc: 0.2810\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 2s 281us/step - loss: 1.8757 - acc: 0.2319 - val_loss: 1.8398 - val_acc: 0.2910\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 2s 286us/step - loss: 1.8694 - acc: 0.2339 - val_loss: 1.8263 - val_acc: 0.2980\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 3s 380us/step - loss: 1.8553 - acc: 0.2491 - val_loss: 1.8125 - val_acc: 0.3240\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 3s 426us/step - loss: 1.8454 - acc: 0.2555 - val_loss: 1.7965 - val_acc: 0.3320\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 1.8344 - acc: 0.2635 - val_loss: 1.7805 - val_acc: 0.3570\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 1.8175 - acc: 0.2745 - val_loss: 1.7630 - val_acc: 0.3710\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 2s 325us/step - loss: 1.8076 - acc: 0.2787 - val_loss: 1.7447 - val_acc: 0.3820\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 2s 282us/step - loss: 1.7914 - acc: 0.2956 - val_loss: 1.7258 - val_acc: 0.3860\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 2s 288us/step - loss: 1.7853 - acc: 0.2907 - val_loss: 1.7053 - val_acc: 0.4020\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 1.7707 - acc: 0.3104 - val_loss: 1.6868 - val_acc: 0.4290\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.7547 - acc: 0.3120 - val_loss: 1.6660 - val_acc: 0.4380\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 2s 325us/step - loss: 1.7298 - acc: 0.3261 - val_loss: 1.6440 - val_acc: 0.4520\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.7254 - acc: 0.3248 - val_loss: 1.6261 - val_acc: 0.4750\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 2s 305us/step - loss: 1.7094 - acc: 0.3403 - val_loss: 1.6049 - val_acc: 0.4890\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 2s 332us/step - loss: 1.6981 - acc: 0.3355 - val_loss: 1.5854 - val_acc: 0.5080\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 2s 296us/step - loss: 1.6783 - acc: 0.3520 - val_loss: 1.5645 - val_acc: 0.5250\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 3s 365us/step - loss: 1.6696 - acc: 0.3568 - val_loss: 1.5444 - val_acc: 0.5320\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 3s 368us/step - loss: 1.6491 - acc: 0.3672 - val_loss: 1.5210 - val_acc: 0.5480\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 3s 339us/step - loss: 1.6237 - acc: 0.3835 - val_loss: 1.4958 - val_acc: 0.5480\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 3s 423us/step - loss: 1.6215 - acc: 0.3784 - val_loss: 1.4758 - val_acc: 0.5660\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.6058 - acc: 0.3856 - val_loss: 1.4525 - val_acc: 0.5670\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 2s 242us/step - loss: 1.5778 - acc: 0.3984 - val_loss: 1.4316 - val_acc: 0.5830\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.5783 - acc: 0.3953 - val_loss: 1.4111 - val_acc: 0.5960\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 1.5507 - acc: 0.4112 - val_loss: 1.3914 - val_acc: 0.6040\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 1.5323 - acc: 0.4285 - val_loss: 1.3661 - val_acc: 0.6150\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 2s 258us/step - loss: 1.5249 - acc: 0.4240 - val_loss: 1.3460 - val_acc: 0.6180\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 2s 285us/step - loss: 1.5086 - acc: 0.4177 - val_loss: 1.3272 - val_acc: 0.6340\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.4862 - acc: 0.4381 - val_loss: 1.3040 - val_acc: 0.6380\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 2s 242us/step - loss: 1.4786 - acc: 0.4444 - val_loss: 1.2840 - val_acc: 0.6420\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 1.4588 - acc: 0.4495 - val_loss: 1.2647 - val_acc: 0.6500\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 2s 240us/step - loss: 1.4520 - acc: 0.4491 - val_loss: 1.2475 - val_acc: 0.6620\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 1.4430 - acc: 0.4545 - val_loss: 1.2290 - val_acc: 0.6680\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 2s 283us/step - loss: 1.4187 - acc: 0.4749 - val_loss: 1.2119 - val_acc: 0.6720\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.4082 - acc: 0.4737 - val_loss: 1.1942 - val_acc: 0.6760\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 2s 266us/step - loss: 1.4009 - acc: 0.4744 - val_loss: 1.1774 - val_acc: 0.6830\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 1.3939 - acc: 0.4777 - val_loss: 1.1631 - val_acc: 0.6810\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 1.3706 - acc: 0.4864 - val_loss: 1.1477 - val_acc: 0.6870\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 2s 281us/step - loss: 1.3501 - acc: 0.4911 - val_loss: 1.1298 - val_acc: 0.6890\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 1.3428 - acc: 0.5011 - val_loss: 1.1135 - val_acc: 0.6930\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 2s 272us/step - loss: 1.3302 - acc: 0.4987 - val_loss: 1.0963 - val_acc: 0.6950\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 2s 255us/step - loss: 1.3303 - acc: 0.5008 - val_loss: 1.0833 - val_acc: 0.6970\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 1.3126 - acc: 0.5061 - val_loss: 1.0688 - val_acc: 0.6980\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 2s 268us/step - loss: 1.3142 - acc: 0.5119 - val_loss: 1.0550 - val_acc: 0.6990\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 3s 343us/step - loss: 1.2975 - acc: 0.5201 - val_loss: 1.0435 - val_acc: 0.7000\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 2s 315us/step - loss: 1.2809 - acc: 0.5168 - val_loss: 1.0304 - val_acc: 0.6950\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 3s 343us/step - loss: 1.2533 - acc: 0.5307 - val_loss: 1.0175 - val_acc: 0.7040\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 2s 331us/step - loss: 1.2653 - acc: 0.5315 - val_loss: 1.0062 - val_acc: 0.7020\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 3s 377us/step - loss: 1.2534 - acc: 0.5215 - val_loss: 0.9971 - val_acc: 0.7100\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 2s 275us/step - loss: 1.2366 - acc: 0.5419 - val_loss: 0.9866 - val_acc: 0.7080\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 2s 275us/step - loss: 1.2314 - acc: 0.5412 - val_loss: 0.9768 - val_acc: 0.7130\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 1.2109 - acc: 0.5459 - val_loss: 0.9631 - val_acc: 0.7110\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 3s 370us/step - loss: 1.2051 - acc: 0.5504 - val_loss: 0.9527 - val_acc: 0.7100\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 3s 386us/step - loss: 1.2140 - acc: 0.5396 - val_loss: 0.9452 - val_acc: 0.7100\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 2s 304us/step - loss: 1.1815 - acc: 0.5640 - val_loss: 0.9314 - val_acc: 0.7150\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 1.1776 - acc: 0.5619 - val_loss: 0.9232 - val_acc: 0.7170\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 2s 324us/step - loss: 1.1828 - acc: 0.5600 - val_loss: 0.9176 - val_acc: 0.7180\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 2s 243us/step - loss: 1.1619 - acc: 0.5680 - val_loss: 0.9063 - val_acc: 0.7200\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 2s 303us/step - loss: 1.1520 - acc: 0.5763 - val_loss: 0.8983 - val_acc: 0.7180\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 1.1530 - acc: 0.5779 - val_loss: 0.8908 - val_acc: 0.7210\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 2s 330us/step - loss: 1.1379 - acc: 0.5747 - val_loss: 0.8808 - val_acc: 0.7290\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 2s 318us/step - loss: 1.1344 - acc: 0.5784 - val_loss: 0.8737 - val_acc: 0.7270\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 3s 397us/step - loss: 1.1259 - acc: 0.5816 - val_loss: 0.8659 - val_acc: 0.7250\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 1.1236 - acc: 0.5813 - val_loss: 0.8587 - val_acc: 0.7300\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 2s 302us/step - loss: 1.1064 - acc: 0.5928 - val_loss: 0.8530 - val_acc: 0.7310\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 2s 317us/step - loss: 1.1066 - acc: 0.5869 - val_loss: 0.8435 - val_acc: 0.7320\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 2s 286us/step - loss: 1.0996 - acc: 0.5883 - val_loss: 0.8373 - val_acc: 0.7290\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 2s 275us/step - loss: 1.1040 - acc: 0.5912 - val_loss: 0.8332 - val_acc: 0.7340\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 1.0815 - acc: 0.6021 - val_loss: 0.8269 - val_acc: 0.7330\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 2s 261us/step - loss: 1.0806 - acc: 0.5999 - val_loss: 0.8212 - val_acc: 0.7320\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 3s 396us/step - loss: 1.0701 - acc: 0.6007 - val_loss: 0.8159 - val_acc: 0.7310\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 4s 488us/step - loss: 1.0615 - acc: 0.6104 - val_loss: 0.8083 - val_acc: 0.7360\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 3s 434us/step - loss: 1.0603 - acc: 0.6068 - val_loss: 0.8040 - val_acc: 0.7360\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 3s 382us/step - loss: 1.0485 - acc: 0.6136 - val_loss: 0.7981 - val_acc: 0.7340\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 4s 520us/step - loss: 1.0488 - acc: 0.6128 - val_loss: 0.7929 - val_acc: 0.7370\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 3s 419us/step - loss: 1.0385 - acc: 0.6217 - val_loss: 0.7897 - val_acc: 0.7360\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 3s 433us/step - loss: 1.0415 - acc: 0.6136 - val_loss: 0.7852 - val_acc: 0.7390\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 4s 575us/step - loss: 1.0344 - acc: 0.6231 - val_loss: 0.7816 - val_acc: 0.7360\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 4s 492us/step - loss: 1.0280 - acc: 0.6200 - val_loss: 0.7740 - val_acc: 0.7340\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 1.0211 - acc: 0.6217 - val_loss: 0.7711 - val_acc: 0.7350\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 2s 279us/step - loss: 1.0228 - acc: 0.6224 - val_loss: 0.7648 - val_acc: 0.7390\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 2s 280us/step - loss: 1.0261 - acc: 0.6199 - val_loss: 0.7627 - val_acc: 0.7400\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 2s 286us/step - loss: 1.0131 - acc: 0.6215 - val_loss: 0.7602 - val_acc: 0.7430\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 3s 343us/step - loss: 1.0185 - acc: 0.6323 - val_loss: 0.7568 - val_acc: 0.7410\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 3s 346us/step - loss: 1.0114 - acc: 0.6264 - val_loss: 0.7523 - val_acc: 0.7430\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 3s 343us/step - loss: 1.0060 - acc: 0.6159 - val_loss: 0.7504 - val_acc: 0.7400\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 2s 325us/step - loss: 0.9894 - acc: 0.6336 - val_loss: 0.7444 - val_acc: 0.7450\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 0.9961 - acc: 0.6303 - val_loss: 0.7404 - val_acc: 0.7430\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 2s 289us/step - loss: 0.9927 - acc: 0.6235 - val_loss: 0.7375 - val_acc: 0.7470\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 2s 318us/step - loss: 0.9866 - acc: 0.6377 - val_loss: 0.7346 - val_acc: 0.7440\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 0.9881 - acc: 0.6329 - val_loss: 0.7324 - val_acc: 0.7470\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 3s 334us/step - loss: 0.9794 - acc: 0.6435 - val_loss: 0.7303 - val_acc: 0.7430\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 2s 311us/step - loss: 0.9638 - acc: 0.6491 - val_loss: 0.7258 - val_acc: 0.7470\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 3s 432us/step - loss: 0.9605 - acc: 0.6449 - val_loss: 0.7212 - val_acc: 0.7470\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 3s 446us/step - loss: 0.9634 - acc: 0.6433 - val_loss: 0.7188 - val_acc: 0.7450\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 0.9576 - acc: 0.6447 - val_loss: 0.7162 - val_acc: 0.7470\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 4s 500us/step - loss: 0.9607 - acc: 0.6375 - val_loss: 0.7137 - val_acc: 0.7490\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 3s 439us/step - loss: 0.9472 - acc: 0.6499 - val_loss: 0.7111 - val_acc: 0.7460\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 3s 336us/step - loss: 0.9360 - acc: 0.6591 - val_loss: 0.7075 - val_acc: 0.7510\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 4s 524us/step - loss: 0.9584 - acc: 0.6417 - val_loss: 0.7068 - val_acc: 0.7470\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 3s 372us/step - loss: 0.9395 - acc: 0.6583 - val_loss: 0.7048 - val_acc: 0.7480\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 3s 359us/step - loss: 0.9473 - acc: 0.6471 - val_loss: 0.7035 - val_acc: 0.7480\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 3s 368us/step - loss: 0.9271 - acc: 0.6607 - val_loss: 0.7000 - val_acc: 0.7520\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 2s 319us/step - loss: 0.9357 - acc: 0.6559 - val_loss: 0.6994 - val_acc: 0.7500\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 3s 342us/step - loss: 0.9277 - acc: 0.6579 - val_loss: 0.6961 - val_acc: 0.7500\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 4s 478us/step - loss: 0.9123 - acc: 0.6624 - val_loss: 0.6939 - val_acc: 0.7500\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 3s 419us/step - loss: 0.9252 - acc: 0.6576 - val_loss: 0.6911 - val_acc: 0.7510\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 3s 357us/step - loss: 0.9122 - acc: 0.6635 - val_loss: 0.6886 - val_acc: 0.7520\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 3s 359us/step - loss: 0.9127 - acc: 0.6609 - val_loss: 0.6851 - val_acc: 0.7520\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 3s 345us/step - loss: 0.9154 - acc: 0.6695 - val_loss: 0.6857 - val_acc: 0.7520\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 3s 336us/step - loss: 0.8925 - acc: 0.6673 - val_loss: 0.6822 - val_acc: 0.7510\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 3s 336us/step - loss: 0.9016 - acc: 0.6691 - val_loss: 0.6800 - val_acc: 0.7530\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 3s 338us/step - loss: 0.9012 - acc: 0.6656 - val_loss: 0.6811 - val_acc: 0.7540\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 0.8819 - acc: 0.6695 - val_loss: 0.6774 - val_acc: 0.7500\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 0.9036 - acc: 0.6663 - val_loss: 0.6762 - val_acc: 0.7510\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 3s 350us/step - loss: 0.8850 - acc: 0.6735 - val_loss: 0.6729 - val_acc: 0.7530\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 3s 350us/step - loss: 0.8854 - acc: 0.6700 - val_loss: 0.6691 - val_acc: 0.7520\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 3s 393us/step - loss: 0.8868 - acc: 0.6732 - val_loss: 0.6671 - val_acc: 0.7530\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 2s 312us/step - loss: 0.8848 - acc: 0.6736 - val_loss: 0.6663 - val_acc: 0.7520\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 2s 275us/step - loss: 0.8929 - acc: 0.6699 - val_loss: 0.6658 - val_acc: 0.7520\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 2s 301us/step - loss: 0.8752 - acc: 0.6741 - val_loss: 0.6639 - val_acc: 0.7530\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.8828 - acc: 0.6736 - val_loss: 0.6634 - val_acc: 0.7510\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 2s 323us/step - loss: 0.8732 - acc: 0.6769 - val_loss: 0.6614 - val_acc: 0.7520\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 3s 339us/step - loss: 0.8906 - acc: 0.6764 - val_loss: 0.6612 - val_acc: 0.7540\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 0.8581 - acc: 0.6844 - val_loss: 0.6567 - val_acc: 0.7510\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 2s 289us/step - loss: 0.8604 - acc: 0.6864 - val_loss: 0.6547 - val_acc: 0.7540\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 2s 282us/step - loss: 0.8660 - acc: 0.6757 - val_loss: 0.6567 - val_acc: 0.7540\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 2s 275us/step - loss: 0.8642 - acc: 0.6811 - val_loss: 0.6535 - val_acc: 0.7540\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 2s 293us/step - loss: 0.8621 - acc: 0.6781 - val_loss: 0.6537 - val_acc: 0.7580\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 2s 309us/step - loss: 0.8671 - acc: 0.6801 - val_loss: 0.6530 - val_acc: 0.7550\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 2s 274us/step - loss: 0.8507 - acc: 0.6872 - val_loss: 0.6506 - val_acc: 0.7570\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 0.8666 - acc: 0.6807 - val_loss: 0.6508 - val_acc: 0.7560\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 2s 294us/step - loss: 0.8525 - acc: 0.6841 - val_loss: 0.6498 - val_acc: 0.7560\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 2s 284us/step - loss: 0.8526 - acc: 0.6873 - val_loss: 0.6494 - val_acc: 0.7570\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 0.8349 - acc: 0.6873 - val_loss: 0.6462 - val_acc: 0.7570\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 2s 290us/step - loss: 0.8395 - acc: 0.6943 - val_loss: 0.6438 - val_acc: 0.7550\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 2s 279us/step - loss: 0.8597 - acc: 0.6856 - val_loss: 0.6460 - val_acc: 0.7550\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 2s 279us/step - loss: 0.8350 - acc: 0.6899 - val_loss: 0.6420 - val_acc: 0.7570\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 3s 357us/step - loss: 0.8306 - acc: 0.6916 - val_loss: 0.6408 - val_acc: 0.7540\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 2s 300us/step - loss: 0.8376 - acc: 0.6868 - val_loss: 0.6390 - val_acc: 0.7550\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 2s 308us/step - loss: 0.8348 - acc: 0.6925 - val_loss: 0.6380 - val_acc: 0.7560\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 2s 297us/step - loss: 0.8269 - acc: 0.6967 - val_loss: 0.6359 - val_acc: 0.7550\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 0.8168 - acc: 0.7023 - val_loss: 0.6345 - val_acc: 0.7560\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 2s 251us/step - loss: 0.8265 - acc: 0.6987 - val_loss: 0.6325 - val_acc: 0.7600\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.8232 - acc: 0.6923 - val_loss: 0.6350 - val_acc: 0.7540\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.8209 - acc: 0.6971 - val_loss: 0.6349 - val_acc: 0.7570\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 2s 283us/step - loss: 0.8302 - acc: 0.6937 - val_loss: 0.6337 - val_acc: 0.7580\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 0.8153 - acc: 0.6979 - val_loss: 0.6330 - val_acc: 0.7510\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.8184 - acc: 0.7007 - val_loss: 0.6336 - val_acc: 0.7550\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.8060 - acc: 0.6972 - val_loss: 0.6300 - val_acc: 0.7570\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 2s 240us/step - loss: 0.8092 - acc: 0.6977 - val_loss: 0.6305 - val_acc: 0.7560\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 2s 247us/step - loss: 0.8070 - acc: 0.7029 - val_loss: 0.6281 - val_acc: 0.7570\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.8084 - acc: 0.7008 - val_loss: 0.6289 - val_acc: 0.7570\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 2s 266us/step - loss: 0.8038 - acc: 0.7011 - val_loss: 0.6265 - val_acc: 0.7600\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 2s 276us/step - loss: 0.7957 - acc: 0.7049 - val_loss: 0.6250 - val_acc: 0.7560\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.8027 - acc: 0.7009 - val_loss: 0.6234 - val_acc: 0.7620\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.8149 - acc: 0.6981 - val_loss: 0.6245 - val_acc: 0.7600\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.7981 - acc: 0.7099 - val_loss: 0.6233 - val_acc: 0.7620\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.7970 - acc: 0.7043 - val_loss: 0.6215 - val_acc: 0.7630\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 2s 271us/step - loss: 0.8062 - acc: 0.7016 - val_loss: 0.6231 - val_acc: 0.7630\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 0.7919 - acc: 0.7072 - val_loss: 0.6238 - val_acc: 0.7590\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 0.8034 - acc: 0.7035 - val_loss: 0.6222 - val_acc: 0.7650\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 2s 290us/step - loss: 0.7837 - acc: 0.7069 - val_loss: 0.6191 - val_acc: 0.7630\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 2s 291us/step - loss: 0.7794 - acc: 0.7081 - val_loss: 0.6206 - val_acc: 0.7630\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 2s 278us/step - loss: 0.7895 - acc: 0.7045 - val_loss: 0.6198 - val_acc: 0.7570\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 2s 270us/step - loss: 0.7949 - acc: 0.7039 - val_loss: 0.6192 - val_acc: 0.7630\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.7885 - acc: 0.7088 - val_loss: 0.6186 - val_acc: 0.7620\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 2s 236us/step - loss: 0.7907 - acc: 0.7105 - val_loss: 0.6185 - val_acc: 0.7620\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 2s 240us/step - loss: 0.7737 - acc: 0.7143 - val_loss: 0.6167 - val_acc: 0.7630\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 0.7725 - acc: 0.7107 - val_loss: 0.6159 - val_acc: 0.7610\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 2s 299us/step - loss: 0.7768 - acc: 0.7103 - val_loss: 0.6152 - val_acc: 0.7640\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 0.7732 - acc: 0.7136 - val_loss: 0.6135 - val_acc: 0.7650\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.7688 - acc: 0.7141 - val_loss: 0.6124 - val_acc: 0.7630\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 2s 321us/step - loss: 0.7758 - acc: 0.7105 - val_loss: 0.6119 - val_acc: 0.7620\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 2s 274us/step - loss: 0.7721 - acc: 0.7147 - val_loss: 0.6116 - val_acc: 0.7630\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 2s 265us/step - loss: 0.7758 - acc: 0.7160 - val_loss: 0.6126 - val_acc: 0.7620\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 2s 265us/step - loss: 0.7729 - acc: 0.7129 - val_loss: 0.6111 - val_acc: 0.7600\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 2s 244us/step - loss: 0.7640 - acc: 0.7193 - val_loss: 0.6114 - val_acc: 0.7640\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 2s 333us/step - loss: 0.7607 - acc: 0.7151 - val_loss: 0.6101 - val_acc: 0.7670\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 2s 314us/step - loss: 0.7609 - acc: 0.7207 - val_loss: 0.6080 - val_acc: 0.7670\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 2s 278us/step - loss: 0.7588 - acc: 0.7213 - val_loss: 0.6069 - val_acc: 0.7640\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 2s 242us/step - loss: 0.7584 - acc: 0.7207 - val_loss: 0.6071 - val_acc: 0.7640\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 2s 254us/step - loss: 0.7495 - acc: 0.7212 - val_loss: 0.6053 - val_acc: 0.7640\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 2s 271us/step - loss: 0.7445 - acc: 0.7237 - val_loss: 0.6060 - val_acc: 0.7640\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 2s 246us/step - loss: 0.7427 - acc: 0.7223 - val_loss: 0.6050 - val_acc: 0.7650\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 0.7585 - acc: 0.7179 - val_loss: 0.6033 - val_acc: 0.7670\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 0.7580 - acc: 0.7155 - val_loss: 0.6030 - val_acc: 0.7670\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 2s 252us/step - loss: 0.7451 - acc: 0.7205 - val_loss: 0.6027 - val_acc: 0.7640\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 2s 248us/step - loss: 0.7456 - acc: 0.7248 - val_loss: 0.6028 - val_acc: 0.7640\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                          label_train_final,\n",
    "                          epochs=200,\n",
    "                          batch_size=256,\n",
    "                          validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 71us/step\n",
      "1500/1500 [==============================] - 0s 91us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46537125782966615, 0.8260000000317892]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6427671144803365, 0.7519999998410543]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "# one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "# one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train-test-split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "# Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 4s 109us/step - loss: 1.8992 - acc: 0.1994 - val_loss: 1.8547 - val_acc: 0.2330\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 2s 73us/step - loss: 1.7771 - acc: 0.2924 - val_loss: 1.6905 - val_acc: 0.3717\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 1.5670 - acc: 0.4490 - val_loss: 1.4548 - val_acc: 0.5080\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 1.3304 - acc: 0.5683 - val_loss: 1.2314 - val_acc: 0.6047\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 1.1344 - acc: 0.6389 - val_loss: 1.0591 - val_acc: 0.6577\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.9898 - acc: 0.6787 - val_loss: 0.9361 - val_acc: 0.6893\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.8866 - acc: 0.7032 - val_loss: 0.8514 - val_acc: 0.7027\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.8129 - acc: 0.7196 - val_loss: 0.7895 - val_acc: 0.7183\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.7599 - acc: 0.7328 - val_loss: 0.7458 - val_acc: 0.7307\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.7203 - acc: 0.7415 - val_loss: 0.7134 - val_acc: 0.7390\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.6896 - acc: 0.7483 - val_loss: 0.6899 - val_acc: 0.7443\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.6656 - acc: 0.7549 - val_loss: 0.6708 - val_acc: 0.7487\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.6456 - acc: 0.7609 - val_loss: 0.6552 - val_acc: 0.7503\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.6289 - acc: 0.7660 - val_loss: 0.6429 - val_acc: 0.7570\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.6144 - acc: 0.7701 - val_loss: 0.6311 - val_acc: 0.7620\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.6019 - acc: 0.7747 - val_loss: 0.6220 - val_acc: 0.7663\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5907 - acc: 0.7787 - val_loss: 0.6143 - val_acc: 0.7670\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5804 - acc: 0.7828 - val_loss: 0.6062 - val_acc: 0.7727\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 3s 76us/step - loss: 0.5709 - acc: 0.7872 - val_loss: 0.6002 - val_acc: 0.7747\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.5627 - acc: 0.7898 - val_loss: 0.5961 - val_acc: 0.7750\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.5546 - acc: 0.7926 - val_loss: 0.5903 - val_acc: 0.7780\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5470 - acc: 0.7954 - val_loss: 0.5876 - val_acc: 0.7770\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.5402 - acc: 0.7985 - val_loss: 0.5825 - val_acc: 0.7813\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5333 - acc: 0.8011 - val_loss: 0.5808 - val_acc: 0.7823\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.5276 - acc: 0.8033 - val_loss: 0.5726 - val_acc: 0.7883\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.5216 - acc: 0.8074 - val_loss: 0.5713 - val_acc: 0.7880\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.5158 - acc: 0.8065 - val_loss: 0.5688 - val_acc: 0.7907\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.5103 - acc: 0.8097 - val_loss: 0.5660 - val_acc: 0.7880\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.5052 - acc: 0.8121 - val_loss: 0.5626 - val_acc: 0.7917\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.5000 - acc: 0.8156 - val_loss: 0.5588 - val_acc: 0.7930\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 2s 65us/step - loss: 0.4954 - acc: 0.8168 - val_loss: 0.5562 - val_acc: 0.7973\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.4906 - acc: 0.8191 - val_loss: 0.5562 - val_acc: 0.7973\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.4864 - acc: 0.8216 - val_loss: 0.5518 - val_acc: 0.8050\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 2s 64us/step - loss: 0.4819 - acc: 0.8238 - val_loss: 0.5500 - val_acc: 0.8027\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.4777 - acc: 0.8256 - val_loss: 0.5484 - val_acc: 0.8030\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.4737 - acc: 0.8256 - val_loss: 0.5461 - val_acc: 0.8043\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4695 - acc: 0.8282 - val_loss: 0.5455 - val_acc: 0.8043\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4660 - acc: 0.8298 - val_loss: 0.5435 - val_acc: 0.8047\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4622 - acc: 0.8320 - val_loss: 0.5432 - val_acc: 0.8073\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.4584 - acc: 0.8337 - val_loss: 0.5422 - val_acc: 0.8060\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 2s 67us/step - loss: 0.4551 - acc: 0.8354 - val_loss: 0.5398 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4516 - acc: 0.8356 - val_loss: 0.5386 - val_acc: 0.8083\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.4481 - acc: 0.8366 - val_loss: 0.5382 - val_acc: 0.8110\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.4449 - acc: 0.8381 - val_loss: 0.5385 - val_acc: 0.8110\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4419 - acc: 0.8396 - val_loss: 0.5350 - val_acc: 0.8143\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 2s 68us/step - loss: 0.4386 - acc: 0.8418 - val_loss: 0.5346 - val_acc: 0.8163\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.4358 - acc: 0.8420 - val_loss: 0.5368 - val_acc: 0.8120\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 2s 62us/step - loss: 0.4330 - acc: 0.8432 - val_loss: 0.5328 - val_acc: 0.8160\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.4300 - acc: 0.8439 - val_loss: 0.5323 - val_acc: 0.8180\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 2s 65us/step - loss: 0.4274 - acc: 0.8449 - val_loss: 0.5318 - val_acc: 0.8170\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 2s 62us/step - loss: 0.4243 - acc: 0.8459 - val_loss: 0.5325 - val_acc: 0.8183\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4220 - acc: 0.8474 - val_loss: 0.5332 - val_acc: 0.8147\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4190 - acc: 0.8483 - val_loss: 0.5319 - val_acc: 0.8187\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 2s 68us/step - loss: 0.4166 - acc: 0.8491 - val_loss: 0.5302 - val_acc: 0.8180\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.4142 - acc: 0.8496 - val_loss: 0.5330 - val_acc: 0.8173\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 2s 68us/step - loss: 0.4117 - acc: 0.8514 - val_loss: 0.5301 - val_acc: 0.8180\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.4092 - acc: 0.8526 - val_loss: 0.5303 - val_acc: 0.8180\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4066 - acc: 0.8534 - val_loss: 0.5322 - val_acc: 0.8180\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4046 - acc: 0.8542 - val_loss: 0.5320 - val_acc: 0.8187\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.4024 - acc: 0.8550 - val_loss: 0.5299 - val_acc: 0.8177\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4001 - acc: 0.8559 - val_loss: 0.5319 - val_acc: 0.8187\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3977 - acc: 0.8567 - val_loss: 0.5307 - val_acc: 0.8177\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3958 - acc: 0.8575 - val_loss: 0.5300 - val_acc: 0.8187\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3934 - acc: 0.8577 - val_loss: 0.5300 - val_acc: 0.8180\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3916 - acc: 0.8591 - val_loss: 0.5300 - val_acc: 0.8163\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3897 - acc: 0.8598 - val_loss: 0.5298 - val_acc: 0.8183\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3873 - acc: 0.8611 - val_loss: 0.5300 - val_acc: 0.8177\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3855 - acc: 0.8622 - val_loss: 0.5303 - val_acc: 0.8180\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3837 - acc: 0.8628 - val_loss: 0.5303 - val_acc: 0.8170\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3815 - acc: 0.8636 - val_loss: 0.5298 - val_acc: 0.8170\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3797 - acc: 0.8638 - val_loss: 0.5319 - val_acc: 0.8167\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3778 - acc: 0.8651 - val_loss: 0.5310 - val_acc: 0.8167\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3761 - acc: 0.8658 - val_loss: 0.5310 - val_acc: 0.8173\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3742 - acc: 0.8668 - val_loss: 0.5326 - val_acc: 0.8157\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3730 - acc: 0.8673 - val_loss: 0.5342 - val_acc: 0.8143\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3707 - acc: 0.8681 - val_loss: 0.5332 - val_acc: 0.8167\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3689 - acc: 0.8685 - val_loss: 0.5325 - val_acc: 0.8163\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3676 - acc: 0.8691 - val_loss: 0.5322 - val_acc: 0.8160\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3658 - acc: 0.8698 - val_loss: 0.5332 - val_acc: 0.8140\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3640 - acc: 0.8705 - val_loss: 0.5340 - val_acc: 0.8150\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3624 - acc: 0.8714 - val_loss: 0.5333 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3607 - acc: 0.8725 - val_loss: 0.5350 - val_acc: 0.8140\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3592 - acc: 0.8717 - val_loss: 0.5399 - val_acc: 0.8117\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3574 - acc: 0.8732 - val_loss: 0.5339 - val_acc: 0.8157\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3561 - acc: 0.8742 - val_loss: 0.5352 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3546 - acc: 0.8745 - val_loss: 0.5368 - val_acc: 0.8140\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3533 - acc: 0.8744 - val_loss: 0.5388 - val_acc: 0.8143\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3515 - acc: 0.8753 - val_loss: 0.5405 - val_acc: 0.8117\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 2s 63us/step - loss: 0.3501 - acc: 0.8773 - val_loss: 0.5371 - val_acc: 0.8140\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3490 - acc: 0.8767 - val_loss: 0.5388 - val_acc: 0.8170\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3472 - acc: 0.8775 - val_loss: 0.5407 - val_acc: 0.8157\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3457 - acc: 0.8786 - val_loss: 0.5388 - val_acc: 0.8147\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3447 - acc: 0.8789 - val_loss: 0.5391 - val_acc: 0.8167\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3430 - acc: 0.8803 - val_loss: 0.5395 - val_acc: 0.8157\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3417 - acc: 0.8796 - val_loss: 0.5435 - val_acc: 0.8163\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3401 - acc: 0.8807 - val_loss: 0.5425 - val_acc: 0.8140\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 2s 63us/step - loss: 0.3391 - acc: 0.8814 - val_loss: 0.5456 - val_acc: 0.8133\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3377 - acc: 0.8812 - val_loss: 0.5452 - val_acc: 0.8123\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3362 - acc: 0.8819 - val_loss: 0.5430 - val_acc: 0.8133\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3350 - acc: 0.8825 - val_loss: 0.5458 - val_acc: 0.8127\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3338 - acc: 0.8831 - val_loss: 0.5446 - val_acc: 0.8130\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 2s 68us/step - loss: 0.3327 - acc: 0.8836 - val_loss: 0.5465 - val_acc: 0.8123\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 2s 72us/step - loss: 0.3309 - acc: 0.8847 - val_loss: 0.5466 - val_acc: 0.8120\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 2s 69us/step - loss: 0.3298 - acc: 0.8839 - val_loss: 0.5487 - val_acc: 0.8117\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 2s 71us/step - loss: 0.3287 - acc: 0.8854 - val_loss: 0.5470 - val_acc: 0.8103\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.3274 - acc: 0.8860 - val_loss: 0.5517 - val_acc: 0.8113\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 2s 64us/step - loss: 0.3263 - acc: 0.8859 - val_loss: 0.5494 - val_acc: 0.8113\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 2s 65us/step - loss: 0.3249 - acc: 0.8862 - val_loss: 0.5515 - val_acc: 0.8123\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 2s 63us/step - loss: 0.3237 - acc: 0.8874 - val_loss: 0.5505 - val_acc: 0.8117\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 2s 74us/step - loss: 0.3229 - acc: 0.8874 - val_loss: 0.5546 - val_acc: 0.8110\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3215 - acc: 0.8882 - val_loss: 0.5521 - val_acc: 0.8117\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3207 - acc: 0.8882 - val_loss: 0.5542 - val_acc: 0.8110\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3193 - acc: 0.8881 - val_loss: 0.5537 - val_acc: 0.8097\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3182 - acc: 0.8894 - val_loss: 0.5571 - val_acc: 0.8090\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3168 - acc: 0.8895 - val_loss: 0.5571 - val_acc: 0.8087\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3158 - acc: 0.8901 - val_loss: 0.5568 - val_acc: 0.8097\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3150 - acc: 0.8903 - val_loss: 0.5562 - val_acc: 0.8093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3136 - acc: 0.8916 - val_loss: 0.5590 - val_acc: 0.8097\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.3127 - acc: 0.8909 - val_loss: 0.5618 - val_acc: 0.8087\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 3s 91us/step - loss: 0.3116 - acc: 0.8923 - val_loss: 0.5606 - val_acc: 0.8083\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                           label_train_final,\n",
    "                           epochs=120,\n",
    "                           batch_size=256,\n",
    "                           validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 3s 82us/step\n",
      "4000/4000 [==============================] - 0s 96us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3064536180676836, 0.8943333333333333]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5693588850498199, 0.80675]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database\n",
    "\n",
    "\n",
    "**Tips For Using Dropout from machinelearningmastery.com**\n",
    "\n",
    "The original paper on Dropout provides experimental results on a suite of standard machine learning problems. As a result they provide a number of useful heuristics to consider when using dropout in practice.\n",
    "\n",
    "* Generally, use a **small dropout value of 20%-50%** of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n",
    "* **Use a larger network.** You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
    "* **Use dropout on incoming (visible) as well as hidden units.** Application of dropout at each layer of the network has shown good results.\n",
    "* Use a **large learning rate with decay and a large momentum.** Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n",
    "* **Constrain the size of network weights.** A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
